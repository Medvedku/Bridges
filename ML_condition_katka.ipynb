{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('condition_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"ID mosta\", 'n_2012', 'n_2013', 'n_2014',\n",
    "       'n_2015', 'n_2016', 'n_2017', 'n_2018', 'n_2019', 'n_2020', 'n_2021',\n",
    "       'n_2022', 'n_2023', 'n_length', 'n_width', 'n_region', 'n_class',\n",
    "       'n_years', 'n_material', 'n_type', 'b_change' ]]\n",
    "\n",
    "data_lt = data[data[\"b_change\"] == -1]\n",
    "data_eq = data[data[\"b_change\"] == 0]\n",
    "data_gt = data[data[\"b_change\"] == 1]\n",
    "\n",
    "# Take 1000 random rows from data_eq\n",
    "random_indices = np.random.choice(data_eq.index, size=1000, replace=False)\n",
    "random_data_eq = data_eq.loc[random_indices]\n",
    "\n",
    "# Concatenate random_data_eq with data_gt\n",
    "new_df = pd.concat([random_data_eq, data_gt], ignore_index=True)\n",
    "\n",
    "predict = \"n_2023\"\n",
    "droplist = [\"ID mosta\", 'b_change', predict]\n",
    "\n",
    "X = new_df.drop(droplist, axis=1)\n",
    "y = new_df[predict]\n",
    "\n",
    "n_layers = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to your training data and transform both training and test data\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 695us/step - loss: 0.7798\n",
      "32/32 [==============================] - 0s 616us/step\n",
      "check\n",
      "True     578\n",
      "False    415\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 58.21%\n",
      "201/201 [==============================] - 0s 387us/step\n",
      "check\n",
      "True     4494\n",
      "False    1911\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 70.16%\n",
      "12/12 [==============================] - 0s 506us/step - loss: 0.5386\n",
      "32/32 [==============================] - 0s 444us/step\n",
      "check\n",
      "True     637\n",
      "False    356\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 64.15%\n",
      "201/201 [==============================] - 0s 351us/step\n",
      "check\n",
      "True     4345\n",
      "False    2060\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 67.84%\n",
      "12/12 [==============================] - 0s 465us/step - loss: 0.4082\n",
      "32/32 [==============================] - 0s 512us/step\n",
      "check\n",
      "True     653\n",
      "False    340\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 65.76%\n",
      "201/201 [==============================] - 0s 404us/step\n",
      "check\n",
      "True     3890\n",
      "False    2515\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 60.73%\n",
      "13/13 [==============================] - 0s 685us/step - loss: 0.6216\n",
      "32/32 [==============================] - 0s 422us/step\n",
      "check\n",
      "True     669\n",
      "False    324\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 67.37%\n",
      "201/201 [==============================] - 0s 383us/step\n",
      "check\n",
      "True     4263\n",
      "False    2142\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 66.56%\n",
      "13/13 [==============================] - 0s 523us/step - loss: 0.5995\n",
      "32/32 [==============================] - 0s 367us/step\n",
      "check\n",
      "True     570\n",
      "False    423\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 57.4%\n",
      "201/201 [==============================] - 0s 389us/step\n",
      "check\n",
      "True     4637\n",
      "False    1768\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 72.4%\n",
      "13/13 [==============================] - 0s 491us/step - loss: 0.5050\n",
      "32/32 [==============================] - 0s 464us/step\n",
      "check\n",
      "True     654\n",
      "False    339\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 65.86%\n",
      "201/201 [==============================] - 0s 346us/step\n",
      "check\n",
      "True     4470\n",
      "False    1935\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 69.79%\n",
      "14/14 [==============================] - 0s 486us/step - loss: 0.4841\n",
      "32/32 [==============================] - 0s 364us/step\n",
      "check\n",
      "True     668\n",
      "False    325\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 67.27%\n",
      "201/201 [==============================] - 0s 353us/step\n",
      "check\n",
      "True     4398\n",
      "False    2007\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 68.67%\n",
      "14/14 [==============================] - 0s 398us/step - loss: 0.6659\n",
      "32/32 [==============================] - 0s 375us/step\n",
      "check\n",
      "True     615\n",
      "False    378\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 61.93%\n",
      "201/201 [==============================] - 0s 372us/step\n",
      "check\n",
      "True     4598\n",
      "False    1807\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 71.79%\n",
      "14/14 [==============================] - 0s 640us/step - loss: 0.4497\n",
      "32/32 [==============================] - 0s 477us/step\n",
      "check\n",
      "True     636\n",
      "False    357\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 64.05%\n",
      "201/201 [==============================] - 0s 404us/step\n",
      "check\n",
      "True     4306\n",
      "False    2099\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 67.23%\n"
     ]
    }
   ],
   "source": [
    "for i in range(800, 1201, 50):\n",
    "    data = df[[\"ID mosta\", 'n_2012', 'n_2013', 'n_2014',\n",
    "        'n_2015', 'n_2016', 'n_2017', 'n_2018', 'n_2019', 'n_2020', 'n_2021',\n",
    "        'n_2022', 'n_2023', 'n_length', 'n_width', 'n_region', 'n_class',\n",
    "        'n_years', 'n_material', 'n_type', 'b_change' ]]\n",
    "\n",
    "    data_lt = data[data[\"b_change\"] == -1]\n",
    "    data_eq = data[data[\"b_change\"] == 0]\n",
    "    data_gt = data[data[\"b_change\"] == 1]\n",
    "\n",
    "    # Take 1000 random rows from data_eq\n",
    "    random_indices = np.random.choice(data_eq.index, size=i, replace=False)\n",
    "    random_data_eq = data_eq.loc[random_indices]\n",
    "\n",
    "    # Concatenate random_data_eq with data_gt\n",
    "    new_df = pd.concat([random_data_eq, data_gt], ignore_index=True)\n",
    "\n",
    "    predict = \"n_2023\"\n",
    "    droplist = [\"ID mosta\", 'b_change', predict]\n",
    "\n",
    "    X = new_df.drop(droplist, axis=1)\n",
    "    y = new_df[predict]\n",
    "\n",
    "    n_layers = X.shape[1]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # Create a Min-Max scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to your training data and transform both training and test data\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    #### Delete up until here ################\n",
    "\n",
    "    # Define the model architecture using the number of normalized features\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(n_layers,)),  # Number of input features\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(16, activation='relu'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model on the normalized training data\n",
    "    model.fit(X_train_normalized, y_train, epochs=250, batch_size=8, validation_split=0.1, verbose=False)\n",
    "\n",
    "    # Evaluate the model on the normalized test data\n",
    "    loss = model.evaluate(X_test_normalized, y_test)\n",
    "    # print(f\"Mean Squared Error on Test Data: {loss}\")\n",
    "\n",
    "    #### Delete from here ################\n",
    "\n",
    "    X = data_gt.drop(droplist, axis=1)\n",
    "    y = data_gt[predict]\n",
    "    X_normalized = scaler.transform(X)\n",
    "\n",
    "    # Make predictions on the normalized test data\n",
    "    predictions = model.predict(X_normalized)\n",
    "\n",
    "    # Round the predicted values to zero decimal places and convert to integers\n",
    "    rounded_predictions = np.round(predictions).astype(int)\n",
    "\n",
    "    # Create a DataFrame to compare the actual 'n_2023' values with the model's predictions\n",
    "    comparison_gt = pd.DataFrame({'Actual': y, 'Predicted': rounded_predictions.flatten()})\n",
    "\n",
    "\n",
    "    # Add a new column \"check\" to the comparison DataFrame\n",
    "    comparison_gt['check'] = comparison_gt['Actual'] == comparison_gt['Predicted']\n",
    "\n",
    "    # Display the comparison DataFrame\n",
    "    print(comparison_gt[\"check\"].value_counts())\n",
    "    print(f\"Percentage of correct predictions: {round(comparison_gt['check'].mean()*100, 2)}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X = data_eq.drop(droplist, axis=1)\n",
    "    y = data_eq[predict]\n",
    "    X_normalized = scaler.transform(X)\n",
    "\n",
    "    # Make predictions on the normalized test data\n",
    "    predictions = model.predict(X_normalized)\n",
    "\n",
    "    # Round the predicted values to zero decimal places and convert to integers\n",
    "    rounded_predictions = np.round(predictions).astype(int)\n",
    "\n",
    "    # Create a DataFrame to compare the actual 'n_2023' values with the model's predictions\n",
    "    comparison_df = pd.DataFrame({'Actual': y, 'Predicted': rounded_predictions.flatten()})\n",
    "\n",
    "\n",
    "    # Add a new column \"check\" to the comparison DataFrame\n",
    "    comparison_df['check'] = comparison_df['Actual'] == comparison_df['Predicted']\n",
    "\n",
    "    # Display the comparison DataFrame\n",
    "    print(comparison_df[\"check\"].value_counts())\n",
    "    print(f\"Percentage of correct predictions: {round(comparison_df['check'].mean()*100, 2)}%\")\n",
    "\n",
    "    # Write results to text file\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(f\"Iteration {i}:\\n\")\n",
    "        f.write(f\"Percentage of correct predictions for data_eq: {round(comparison_df['check'].mean()*100, 2)}%\\n\") \n",
    "        f.write(f\"Percentage of correct predictions for data_gt: {round(comparison_gt['check'].mean()*100, 2)}%\\n\") \n",
    "        f.write(\"\\n\")  # Add a newline to separate results for different iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 352us/step\n",
      "check\n",
      "True     646\n",
      "False    347\n",
      "Name: count, dtype: int64\n",
      "Percentage of correct predictions: 65.06%\n"
     ]
    }
   ],
   "source": [
    "# data = df[[\"ID mosta\", 'n_2012', 'n_2013', 'n_2014',\n",
    "#        'n_2015', 'n_2016', 'n_2017', 'n_2018', 'n_2019', 'n_2020', 'n_2021',\n",
    "#        'n_2022', 'n_2023', 'n_length', 'n_width', 'n_region', 'n_class',\n",
    "#        'n_years', 'n_material', 'n_type', 'b_change' ]]\n",
    "\n",
    "# data = data[data[\"b_change\"] == 1]\n",
    "\n",
    "# predict = \"n_2023\"\n",
    "# droplist = [\"ID mosta\", 'b_change', predict]\n",
    "\n",
    "X = data_gt.drop(droplist, axis=1)\n",
    "y = data_gt[predict]\n",
    "X_normalized = scaler.transform(X)\n",
    "\n",
    "# Make predictions on the normalized test data\n",
    "predictions = model.predict(X_normalized)\n",
    "\n",
    "# Round the predicted values to zero decimal places and convert to integers\n",
    "rounded_predictions = np.round(predictions).astype(int)\n",
    "\n",
    "# Create a DataFrame to compare the actual 'n_2023' values with the model's predictions\n",
    "comparison_df = pd.DataFrame({'Actual': y, 'Predicted': rounded_predictions.flatten()})\n",
    "\n",
    "\n",
    "# Add a new column \"check\" to the comparison DataFrame\n",
    "comparison_df['check'] = comparison_df['Actual'] == comparison_df['Predicted']\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(comparison_df[\"check\"].value_counts())\n",
    "print(f\"Percentage of correct predictions: {round(comparison_df['check'].mean()*100, 2)}%\")\n",
    "\n",
    "X = data_eq.drop(droplist, axis=1)\n",
    "y = data_eq[predict]\n",
    "X_normalized = scaler.transform(X)\n",
    "\n",
    "# Make predictions on the normalized test data\n",
    "predictions = model.predict(X_normalized)\n",
    "\n",
    "# Round the predicted values to zero decimal places and convert to integers\n",
    "rounded_predictions = np.round(predictions).astype(int)\n",
    "\n",
    "# Create a DataFrame to compare the actual 'n_2023' values with the model's predictions\n",
    "comparison_df = pd.DataFrame({'Actual': y, 'Predicted': rounded_predictions.flatten()})\n",
    "\n",
    "\n",
    "# Add a new column \"check\" to the comparison DataFrame\n",
    "comparison_df['check'] = comparison_df['Actual'] == comparison_df['Predicted']\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(comparison_df[\"check\"].value_counts())\n",
    "print(f\"Percentage of correct predictions: {round(comparison_df['check'].mean()*100, 2)}%\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 800\n",
      "i = 850\n",
      "i = 900\n",
      "i = 950\n",
      "i = 1000\n",
      "i = 1050\n",
      "i = 1100\n",
      "i = 1150\n",
      "i = 1200\n"
     ]
    }
   ],
   "source": [
    "for i in range(800, 1201, 50):\n",
    "    print(f\"i = {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"ML_sts.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(best_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (4.6.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (from pymongo) (2.4.2)\n",
      "Requirement already satisfied: pandas in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/yaakov/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n",
    "\n",
    "This code cell imports necessary Python libraries and modules for the project. It includes libraries for MongoDB interaction (`pymongo`), numerical operations (`numpy`), time-related functions (`time`), system-related operations (`sys`), and data manipulation (`pandas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB Connection\n",
    "\n",
    "In this code cell, a connection to a MongoDB database is established using a MongoDB URI (`mongo_URI`). It connects to the \"sim-bridge\" database and accesses the \"PRJ-\" collection within that database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_URI = \"mongodb+srv://monitor:kundrovejmamka@xerxes.57jmr.mongodb.net/alfa?retryWrites=true&w=majority\"\n",
    "cluster = MongoClient(mongo_URI)\n",
    "\n",
    "db = cluster[\"sim-bridge\"]\n",
    "col = db[\"PRJ-7\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval and DataFrame Creation\n",
    "\n",
    "This code cell retrieves documents from the MongoDB collection. It specifically looks for documents where the \"measurements\" field exists and the \"meta\" field does not exist. The retrieved documents are stored in an empty list (`document_list`), and then a Pandas DataFrame (`df2`) is created from this list, enabling further data analysis and manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find documents where \"measurements\" exists and \"meta\" does not\n",
    "documents = col.find()\n",
    "\n",
    "# Initialize an empty list to store the documents\n",
    "document_list = []\n",
    "\n",
    "# Iterate through the cursor and store documents in the list\n",
    "for document in documents:\n",
    "    document_list.append(document)\n",
    "\n",
    "# Create a DataFrame from the list of documents\n",
    "df2 = pd.DataFrame(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle\n",
    "\n",
    "### DataFrame Pickling\n",
    "\n",
    "In this code snippet, the `df` DataFrame is copied from `df2`. Then, the `to_pickle` method is used to serialize and save the DataFrame as 'df.pickle' for future storage or retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.copy()\n",
    "\n",
    "# Pickle the DataFrame to a file (e.g., 'df.pickle')\n",
    "df.to_pickle('df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pickled DataFrame\n",
    "\n",
    "In this code snippet, the `pd.read_pickle` function is used to load a previously pickled DataFrame from the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['31357529339808', '44332625541024',\n",
       "       '8e08d85e-05fe-466a-9810-20018f643c92',\n",
       "       '902ddc4c-2131-40b8-a0f8-0d7fd033f9c5',\n",
       "       'aa9fecb1-3220-40f4-9557-8555126eb533'], dtype='<U36')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract \"uuid\" values from dictionaries in the \"meta\" column\n",
    "uuids = [entry.get(\"uuid\", None) for entry in df[\"meta\"]]\n",
    "\n",
    "# Filter out None values and find unique UUIDs\n",
    "unique_uuids = np.unique([uuid for uuid in uuids if uuid is not None])\n",
    "\n",
    "# unique_uuids now contains all unique UUIDs in the DataFrame\n",
    "\n",
    "(unique_uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rows with NaN in \"measurements\" Column\n",
    "\n",
    "In this operation, rows in the DataFrame are filtered to remove those where the \"measurements\" column contains NaN (missing) values. This effectively eliminates rows without data in the \"measurements\" column from the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the \"measurements\" column\n",
    "df.dropna(subset=[\"measurements\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge Data Classification\n",
    "\n",
    "In this code section, the dataset is divided into two separate DataFrames based on the sensors' names associated with each bridge.\n",
    "\n",
    "1. **Sensor Definitions**: First, the sensors for Bridge M022 and Bridge M023 are defined using predefined lists `sensors_M022` and `sensors_M023`, respectively.\n",
    "\n",
    "2. **Classification Function**: The `classify_bridge` function is defined to classify each document in the DataFrame into one of the bridges based on the presence of sensors. It checks which sensors are present in the \"measurements\" column for each document and assigns the document to \"M022\" or \"M023\" accordingly. In cases where sensors don't match either bridge, the document is classified as \"Unknown.\"\n",
    "\n",
    "3. **Applying Classification**: The classification function is applied to each row in the DataFrame, creating a new column called 'Bridge' that indicates the bridge classification for each document.\n",
    "\n",
    "4. **Creating Separate DataFrames**: Finally, two separate DataFrames, `df_M022` and `df_M023`, are created by filtering the original DataFrame based on the 'Bridge' column. These DataFrames contain the data specific to Bridge M022 and Bridge M023, respectively.\n",
    "\n",
    "This code effectively organizes the data into separate DataFrames for analysis and classification based on the sensors associated with each bridge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sensors for each bridge\n",
    "df.iloc[-1][\"measurements\"].keys(), df.iloc[-2][\"measurements\"].keys()\n",
    "sensors_M022 = ['7', '8', '9', '13', '14', '15', '16', '19', '21', '28']\n",
    "sensors_M023 = ['4', '5', '6', '10', '11', '12', '17', '18', '20']\n",
    "\n",
    "# Function to classify a document to a bridge based on available sensors\n",
    "\n",
    "\n",
    "def classify_bridge(row):\n",
    "    sensors_present = list(row[\"measurements\"].keys())\n",
    "    if any(sensor in sensors_present for sensor in sensors_M022):\n",
    "        return \"M022\"\n",
    "    elif any(sensor in sensors_present for sensor in sensors_M023):\n",
    "        return \"M023\"\n",
    "    else:\n",
    "        return \"Unknown\"  # Handle cases where sensors don't match either bridge\n",
    "\n",
    "\n",
    "# Apply the classification function to create a new 'Bridge' column\n",
    "df[\"Bridge\"] = df.apply(classify_bridge, axis=1)\n",
    "\n",
    "# Create separate DataFrames for M022 and M023\n",
    "df_M022 = df[df[\"Bridge\"] == \"M022\"]\n",
    "df_M023 = df[df[\"Bridge\"] == \"M023\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation for Bridge M022 and M023\n",
    "\n",
    "## Bridge M022\n",
    "### Create a Copy of df_M022\n",
    "- A copy of the DataFrame `df_M022` is created to ensure modifications do not affect the original DataFrame.\n",
    "\n",
    "### Define LVDT Sensors for M022\n",
    "- A list `sensors_M022` is defined containing the names of LVDT sensors for Bridge M022.\n",
    "\n",
    "### Iterate Through LVDT Sensors (M022)\n",
    "- The code iterates through the LVDT sensors defined for Bridge M022 (`sensors_M022`).\n",
    "- For each LVDT sensor, it creates two new columns, `d_{sensor}.pv0` and `d_{sensor}.pv1`, in the DataFrame `df_M022`.\n",
    "- The values for these columns are extracted from the \"measurements\" data for each sensor, specifically \"pv0\" and \"pv1\".\n",
    "- If the sensor data is not present in a row, the corresponding columns are filled with `None`.\n",
    "\n",
    "### Create Columns for Temperature Sensor \"21\" (M022)\n",
    "- Two new columns, `t_21.pv1` and `t_21.pv2`, are created in the DataFrame `df_M022` to store data from temperature sensor \"21\".\n",
    "- The values for these columns are extracted from the \"measurements\" data for sensor \"21\", specifically \"pv1\" and \"pv2\".\n",
    "- If the sensor data is not present in a row, the corresponding columns are filled with `None`.\n",
    "\n",
    "## Bridge M023\n",
    "### Create a Copy of df_M023\n",
    "- A copy of the DataFrame `df_M023` is created to ensure modifications do not affect the original DataFrame.\n",
    "\n",
    "### Define LVDT Sensors for M023\n",
    "- A list `sensors_M023` is defined containing the names of LVDT sensors for Bridge M023.\n",
    "\n",
    "### Iterate Through LVDT Sensors (M023)\n",
    "- The code iterates through the LVDT sensors defined for Bridge M023 (`sensors_M023`).\n",
    "- For each LVDT sensor, it creates two new columns, `d_{sensor}.pv0` and `d_{sensor}.pv1`, in the DataFrame `df_M023`.\n",
    "- The values for these columns are extracted from the \"measurements\" data for each sensor, specifically \"pv0\" and \"pv1\".\n",
    "- If the sensor data is not present in a row, the corresponding columns are filled with `None`.\n",
    "\n",
    "### Create Columns for Temperature Sensor \"20\" (M023)\n",
    "- Two new columns, `t_20.pv1` and `t_20.pv2`, are created in the DataFrame `df_M023` to store data from temperature sensor \"20\".\n",
    "- The values for these columns are extracted from the \"measurements\" data for sensor \"20\", specifically \"pv1\" and \"pv2\".\n",
    "- If the sensor data is not present in a row, the corresponding columns are filled with `None`.\n",
    "\n",
    "This code transforms the original DataFrames `df_M022` and `df_M023` by adding columns to store sensor data from LVDT sensors and temperature sensors for both bridges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df_M022\n",
    "df_M022 = df_M022.copy()\n",
    "\n",
    "# Define the sensors for Bridge M022 (LVDT sensors)\n",
    "sensors_M022 = ['7', '8', '9', '13', '14', '15', '16', '19', '28']\n",
    "\n",
    "# Iterate through the LVDT sensors and create columns for pv0 and pv1\n",
    "for sensor in sensors_M022:\n",
    "    df_M022[f\"d_{sensor}.pv0\"] = df_M022.apply(\n",
    "        lambda row: row[\"measurements\"][sensor][\"pv0\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "    df_M022[f\"d_{sensor}.pv1\"] = df_M022.apply(\n",
    "        lambda row: row[\"measurements\"][sensor][\"pv1\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "\n",
    "# Create columns for temperature sensor \"21\" (pv1 and pv2)\n",
    "sensor = '21'\n",
    "df_M022[f\"t_{sensor}.pv1\"] = df_M022.apply(\n",
    "    lambda row: row[\"measurements\"][sensor][\"pv1\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "df_M022[f\"t_{sensor}.pv2\"] = df_M022.apply(\n",
    "    lambda row: row[\"measurements\"][sensor][\"pv2\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "\n",
    "\n",
    "# Create a copy of df_M023\n",
    "df_M023 = df_M023.copy()\n",
    "\n",
    "# Define the sensors for Bridge M023 (LVDT sensors)\n",
    "sensors_M023 = ['4', '5', '6', '10', '11', '12', '17', '18']\n",
    "\n",
    "# Iterate through the LVDT sensors and create columns for pv0 and pv1\n",
    "for sensor in sensors_M023:\n",
    "    df_M023[f\"d_{sensor}.pv0\"] = df_M023.apply(\n",
    "        lambda row: row[\"measurements\"][sensor][\"pv0\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "    df_M023[f\"d_{sensor}.pv1\"] = df_M023.apply(\n",
    "        lambda row: row[\"measurements\"][sensor][\"pv1\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "\n",
    "# Create columns for temperature sensor \"20\" (pv1 and pv2)\n",
    "sensor = '20'\n",
    "df_M023[f\"t_{sensor}.pv1\"] = df_M023.apply(\n",
    "    lambda row: row[\"measurements\"][sensor][\"pv1\"] if sensor in row[\"measurements\"] else None, axis=1)\n",
    "df_M023[f\"t_{sensor}.pv2\"] = df_M023.apply(\n",
    "    lambda row: row[\"measurements\"][sensor][\"pv2\"] if sensor in row[\"measurements\"] else None, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation for Bridge M022 and M023\n",
    "\n",
    "### Common Steps for Both Bridges\n",
    "- Replace the 'time' column with the 'epoch' value.\n",
    "- Drop the 'measurements' column.\n",
    "\n",
    "### Bridge M022\n",
    "- Create a 'battery' column to store battery voltage.\n",
    "- Drop the 'meta' and 'Bridge' columns.\n",
    "\n",
    "### Bridge M023\n",
    "- Create a 'battery' column to store battery voltage.\n",
    "- Drop the 'meta' and 'Bridge' columns.\n",
    "\n",
    "These steps simplify time representation, create a 'battery' column, and remove unnecessary columns in both bridges' data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the 'time' column with the 'epoch' value for Bridge M022\n",
    "df_M022['time'] = df_M022['time'].apply(lambda x: x['server']['epoch'])\n",
    "\n",
    "# Replace the 'time' column with the 'epoch' value for Bridge M023\n",
    "df_M023['time'] = df_M023['time'].apply(lambda x: x['server']['epoch'])\n",
    "\n",
    "# Drop the 'measurements' column from both DataFrames\n",
    "df_M022.drop(columns=['measurements'], inplace=True)\n",
    "df_M023.drop(columns=['measurements'], inplace=True)\n",
    "\n",
    "# Create the 'battery' column in Bridge M022\n",
    "df_M022['battery'] = df_M022['meta'].apply(\n",
    "    lambda x: x['power']['battery']['V'])\n",
    "\n",
    "# Create the 'battery' column in Bridge M023\n",
    "df_M023['battery'] = df_M023['meta'].apply(\n",
    "    lambda x: x['power']['battery']['V'])\n",
    "\n",
    "# Drop the 'meta' and 'Bridge' columns from both DataFrames\n",
    "df_M022.drop(columns=['meta', 'Bridge'], inplace=True)\n",
    "df_M023.drop(columns=['meta', 'Bridge'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LVDT Sensors\n",
    "- New columns ('d_{sensor}') represent the difference between 'pv0' and 'pv1' for each LVDT sensor in both bridges.\n",
    "\n",
    "### Temperature Sensors\n",
    "- New columns ('t_{sensor}') represent the difference between 'pv1' and 'pv2' for each temperature sensor in both bridges.\n",
    "\n",
    "These transformations simplify sensor data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for the difference between pv0 and pv1 for each LVDT sensor in Bridge M022\n",
    "for sensor in ['7', '8', '9', '13', '14', '15', '16', '19', '28']:\n",
    "    df_M022[f'd_{sensor}'] = df_M022[f'd_{sensor}.pv0'] - \\\n",
    "        df_M022[f'd_{sensor}.pv1']\n",
    "\n",
    "# Create new columns for the difference between pv0 and pv1 for each LVDT sensor in Bridge M023\n",
    "for sensor in ['4', '5', '6', '10', '11', '12', '17', '18']:\n",
    "    df_M023[f'd_{sensor}'] = df_M023[f'd_{sensor}.pv0'] - \\\n",
    "        df_M023[f'd_{sensor}.pv1']\n",
    "    \n",
    "# Create new columns for the difference between pv1 and pv2 for each temperature sensor in Bridge M022\n",
    "for sensor in ['21']:\n",
    "    df_M022[f't_{sensor}'] = df_M022[f't_{sensor}.pv1'] - \\\n",
    "        df_M022[f't_{sensor}.pv2']\n",
    "\n",
    "# Create new columns for the difference between pv1 and pv2 for each temperature sensor in Bridge M023\n",
    "for sensor in ['20']:\n",
    "    df_M023[f't_{sensor}'] = df_M023[f't_{sensor}.pv1'] - \\\n",
    "        df_M023[f't_{sensor}.pv2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['_id', 'time', 'd_7.pv0', 'd_7.pv1', 'd_8.pv0', 'd_8.pv1', 'd_9.pv0',\n",
       "        'd_9.pv1', 'd_13.pv0', 'd_13.pv1', 'd_14.pv0', 'd_14.pv1', 'd_15.pv0',\n",
       "        'd_15.pv1', 'd_16.pv0', 'd_16.pv1', 'd_19.pv0', 'd_19.pv1', 'd_28.pv0',\n",
       "        'd_28.pv1', 't_21.pv1', 't_21.pv2', 'battery', 'd_7', 'd_8', 'd_9',\n",
       "        'd_13', 'd_14', 'd_15', 'd_16', 'd_19', 'd_28', 't_21'],\n",
       "       dtype='object'),\n",
       " Index(['_id', 'time', 'd_4.pv0', 'd_4.pv1', 'd_5.pv0', 'd_5.pv1', 'd_6.pv0',\n",
       "        'd_6.pv1', 'd_10.pv0', 'd_10.pv1', 'd_11.pv0', 'd_11.pv1', 'd_12.pv0',\n",
       "        'd_12.pv1', 'd_17.pv0', 'd_17.pv1', 'd_18.pv0', 'd_18.pv1', 't_20.pv1',\n",
       "        't_20.pv2', 'battery', 'd_4', 'd_5', 'd_6', 'd_10', 'd_11', 'd_12',\n",
       "        'd_17', 'd_18', 't_20'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M022.columns, df_M023.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from df_M022 with time below 1691193600\n",
    "df_M022 = df_M022[df_M022['time'] >= 1691539200]\n",
    "\n",
    "# Remove rows from df_M023 with time below 1691193600\n",
    "df_M023 = df_M023[df_M023['time'] >= 1691539200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df_M022 to a CSV file\n",
    "df_M022.to_csv('df_M022.csv', index=False)\n",
    "\n",
    "# Export df_M023 to a CSV file\n",
    "df_M023.to_csv('df_M023.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling (Saving) the DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (pickle) df_M022 to a file\n",
    "df_M022.to_pickle('df_M022.pickle')\n",
    "\n",
    "# Save (pickle) df_M023 to a file\n",
    "df_M023.to_pickle('df_M023.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pickled DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load df_M022 from the pickle file\n",
    "df_M022 = pd.read_pickle('df_M022.pickle')\n",
    "\n",
    "# Load df_M023 from the pickle file\n",
    "df_M023 = pd.read_pickle('df_M023.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14682, 30), (9636, 33))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M023.shape, df_M022.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pickled DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load df_M022 from the pickle file\n",
    "df_M022 = pd.read_pickle('df_M022.pickle')\n",
    "\n",
    "# Load df_M023 from the pickle file\n",
    "df_M023 = pd.read_pickle('df_M023.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the 'time' column to pandas datetime if not already for both DataFrames\n",
    "df_M022['datetime'] = pd.to_datetime(df_M022['time'], unit='s')\n",
    "df_M023['datetime'] = pd.to_datetime(df_M023['time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id                 object\n",
      "time               float64\n",
      "d_7.pv0            float64\n",
      "d_7.pv1            float64\n",
      "d_8.pv0            float64\n",
      "d_8.pv1            float64\n",
      "d_9.pv0            float64\n",
      "d_9.pv1            float64\n",
      "d_13.pv0           float64\n",
      "d_13.pv1           float64\n",
      "d_14.pv0           float64\n",
      "d_14.pv1           float64\n",
      "d_15.pv0           float64\n",
      "d_15.pv1           float64\n",
      "d_16.pv0           float64\n",
      "d_16.pv1           float64\n",
      "d_19.pv0           float64\n",
      "d_19.pv1           float64\n",
      "d_28.pv0           float64\n",
      "d_28.pv1           float64\n",
      "t_21.pv1           float64\n",
      "t_21.pv2           float64\n",
      "battery            float64\n",
      "d_7                float64\n",
      "d_8                float64\n",
      "d_9                float64\n",
      "d_13               float64\n",
      "d_14               float64\n",
      "d_15               float64\n",
      "d_16               float64\n",
      "d_19               float64\n",
      "d_28               float64\n",
      "t_21               float64\n",
      "datetime    datetime64[ns]\n",
      "dtype: object\n",
      "_id                 object\n",
      "time               float64\n",
      "d_4.pv0            float64\n",
      "d_4.pv1            float64\n",
      "d_5.pv0            float64\n",
      "d_5.pv1            float64\n",
      "d_6.pv0            float64\n",
      "d_6.pv1            float64\n",
      "d_10.pv0           float64\n",
      "d_10.pv1           float64\n",
      "d_11.pv0           float64\n",
      "d_11.pv1           float64\n",
      "d_12.pv0           float64\n",
      "d_12.pv1           float64\n",
      "d_17.pv0           float64\n",
      "d_17.pv1           float64\n",
      "d_18.pv0           float64\n",
      "d_18.pv1           float64\n",
      "t_20.pv1           float64\n",
      "t_20.pv2           float64\n",
      "battery            float64\n",
      "d_4                float64\n",
      "d_5                float64\n",
      "d_6                float64\n",
      "d_10               float64\n",
      "d_11               float64\n",
      "d_12               float64\n",
      "d_17               float64\n",
      "d_18               float64\n",
      "t_20               float64\n",
      "datetime    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types of all columns in daily_df_M022\n",
    "print(df_M022.dtypes)\n",
    "\n",
    "# Check data types of all columns in daily_df_M023\n",
    "print(df_M023.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1874\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1874\u001b[0m     res_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49magg_series(ser, alt, preserve_dtype\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1875\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/ops.py:850\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    848\u001b[0m     preserve_dtype \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    852\u001b[0m npvalues \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/ops.py:871\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 871\u001b[0m     res \u001b[39m=\u001b[39m func(group)\n\u001b[1;32m    872\u001b[0m     res \u001b[39m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:2380\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2377\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2378\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2379\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m-> 2380\u001b[0m         alt\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only),\n\u001b[1;32m   2381\u001b[0m         numeric_only\u001b[39m=\u001b[39mnumeric_only,\n\u001b[1;32m   2382\u001b[0m     )\n\u001b[1;32m   2383\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/series.py:6221\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6213\u001b[0m \u001b[39m@doc\u001b[39m(make_doc(\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   6214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[1;32m   6215\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6219\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   6220\u001b[0m ):\n\u001b[0;32m-> 6221\u001b[0m     \u001b[39mreturn\u001b[39;00m NDFrame\u001b[39m.\u001b[39;49mmean(\u001b[39mself\u001b[39;49m, axis, skipna, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/generic.py:11984\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11977\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[1;32m  11978\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m  11979\u001b[0m     axis: Axis \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11982\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m  11983\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m> 11984\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stat_function(\n\u001b[1;32m  11985\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m, nanops\u001b[39m.\u001b[39;49mnanmean, axis, skipna, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m  11986\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/generic.py:11941\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11939\u001b[0m validate_bool_kwarg(skipna, \u001b[39m\"\u001b[39m\u001b[39mskipna\u001b[39m\u001b[39m\"\u001b[39m, none_allowed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m> 11941\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reduce(\n\u001b[1;32m  11942\u001b[0m     func, name\u001b[39m=\u001b[39;49mname, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, numeric_only\u001b[39m=\u001b[39;49mnumeric_only\n\u001b[1;32m  11943\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/series.py:6129\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   6126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSeries.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not allow \u001b[39m\u001b[39m{\u001b[39;00mkwd_name\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mnumeric_only\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6127\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith non-numeric dtypes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6128\u001b[0m     )\n\u001b[0;32m-> 6129\u001b[0m \u001b[39mreturn\u001b[39;00m op(delegate, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[39m=\u001b[39m func(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, mask\u001b[39m=\u001b[39;49mmask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    718\u001b[0m count \u001b[39m=\u001b[39m _get_counts(values\u001b[39m.\u001b[39mshape, mask, axis, dtype\u001b[39m=\u001b[39mdtype_count)\n\u001b[0;32m--> 719\u001b[0m the_sum \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49msum(axis, dtype\u001b[39m=\u001b[39;49mdtype_sum)\n\u001b[1;32m    720\u001b[0m the_sum \u001b[39m=\u001b[39m _ensure_numeric(the_sum)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/numpy/core/_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'ObjectId' and 'ObjectId'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yaakov/Documents/Github/Bridges/Presov/report 1/presov_repot.ipynb Cell 37\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yaakov/Documents/Github/Bridges/Presov/report%201/presov_repot.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Resample the data to daily bins and aggregate using mean (or another aggregation method)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yaakov/Documents/Github/Bridges/Presov/report%201/presov_repot.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Replace 'mean' with another aggregation method if needed\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/yaakov/Documents/Github/Bridges/Presov/report%201/presov_repot.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m daily_df_M022 \u001b[39m=\u001b[39m df_M022\u001b[39m.\u001b[39;49mresample(\u001b[39m'\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m'\u001b[39;49m, on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdatetime\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yaakov/Documents/Github/Bridges/Presov/report%201/presov_repot.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Replace 'mean' with another aggregation method if needed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yaakov/Documents/Github/Bridges/Presov/report%201/presov_repot.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m daily_df_M023 \u001b[39m=\u001b[39m df_M023\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/resample.py:1342\u001b[0m, in \u001b[0;36mResampler.mean\u001b[0;34m(self, numeric_only, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m maybe_warn_args_and_kwargs(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, args, kwargs)\n\u001b[1;32m   1341\u001b[0m nv\u001b[39m.\u001b[39mvalidate_resampler_func(\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1342\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_downsample(\u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m, numeric_only\u001b[39m=\u001b[39;49mnumeric_only)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/resample.py:1728\u001b[0m, in \u001b[0;36mDatetimeIndexResampler._downsample\u001b[0;34m(self, how, **kwargs)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[39m# we are downsampling\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m \u001b[39m# we want to call the actual grouper method here\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1728\u001b[0m     result \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mgroupby(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper)\u001b[39m.\u001b[39;49maggregate(how, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1729\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     \u001b[39m# test_resample_axis1\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m     result \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mgroupby(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper)\u001b[39m.\u001b[39maggregate(how, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1445\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1442\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mengine_kwargs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m engine_kwargs\n\u001b[1;32m   1444\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m-> 1445\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49magg()\n\u001b[1;32m   1446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1447\u001b[0m     \u001b[39m# GH #52849\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mas_index \u001b[39mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/apply.py:172\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_str()\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_dict_like()\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/apply.py:586\u001b[0m, in \u001b[0;36mApply.apply_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39maxis\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis\n\u001b[0;32m--> 586\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_str(obj, func, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/apply.py:669\u001b[0m, in \u001b[0;36mApply._apply_str\u001b[0;34m(self, obj, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, func)\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(f):\n\u001b[0;32m--> 669\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    671\u001b[0m \u001b[39m# people may aggregate on a non-callable attribute\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39m# but don't let them think they can pass args to it\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:2378\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2371\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2372\u001b[0m         grouped_mean,\n\u001b[1;32m   2373\u001b[0m         executor\u001b[39m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2374\u001b[0m         engine_kwargs,\n\u001b[1;32m   2375\u001b[0m         min_periods\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   2376\u001b[0m     )\n\u001b[1;32m   2377\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2378\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_agg_general(\n\u001b[1;32m   2379\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2380\u001b[0m         alt\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only),\n\u001b[1;32m   2381\u001b[0m         numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[1;32m   2382\u001b[0m     )\n\u001b[1;32m   2383\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1929\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim, alt\u001b[39m=\u001b[39malt)\n\u001b[1;32m   1927\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1929\u001b[0m new_mgr \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgrouped_reduce(array_func)\n\u001b[1;32m   1930\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   1931\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/internals/managers.py:1428\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[39mif\u001b[39;00m blk\u001b[39m.\u001b[39mis_object:\n\u001b[1;32m   1425\u001b[0m     \u001b[39m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[39m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[39mfor\u001b[39;00m sb \u001b[39min\u001b[39;00m blk\u001b[39m.\u001b[39m_split():\n\u001b[0;32m-> 1428\u001b[0m         applied \u001b[39m=\u001b[39m sb\u001b[39m.\u001b[39;49mapply(func)\n\u001b[1;32m   1429\u001b[0m         result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1430\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/internals/blocks.py:366\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Block]:\n\u001b[1;32m    362\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    one\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    368\u001b[0m     result \u001b[39m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    369\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1926\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1926\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_agg_py_fallback(how, values, ndim\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim, alt\u001b[39m=\u001b[39;49malt)\n\u001b[1;32m   1927\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/Github/Bridges/Bridges_VENV/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1878\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39magg function failed [how->\u001b[39m\u001b[39m{\u001b[39;00mhow\u001b[39m}\u001b[39;00m\u001b[39m,dtype->\u001b[39m\u001b[39m{\u001b[39;00mser\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1877\u001b[0m     \u001b[39m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1878\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(err)(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m \u001b[39mif\u001b[39;00m ser\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[1;32m   1881\u001b[0m     res_values \u001b[39m=\u001b[39m res_values\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Resample the data to daily bins and aggregate using mean (or another aggregation method)\n",
    "# Replace 'mean' with another aggregation method if needed\n",
    "daily_df_M022 = df_M022.resample('D', on='datetime').mean()\n",
    "# Replace 'mean' with another aggregation method if needed\n",
    "daily_df_M023 = df_M023.resample('D', on='datetime').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8872a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"PRJ-9\"]\n",
    "collection = db[\"M2717\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519faa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of documents\n",
    "total_documents = collection.count_documents({})\n",
    "total_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_info = {\n",
    "    1: {\n",
    "        \"UUID\": \"185284862261152\",\n",
    "        \"SN\": 128,\n",
    "        \"Color\": \"#16cc62\",\n",
    "        \"Sensors\": {\n",
    "            6:  {\"HEX\": \"0x06\", \"Position\": 12, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            10: {\"HEX\": \"0x0A\", \"Position\": 11, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            11: {\"HEX\": \"0x0B\", \"Position\": 15, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            17: {\"HEX\": \"0x11\", \"Position\": 14, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            19: {\"HEX\": \"0x13\", \"Position\": 13, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            20: {\"HEX\": \"0x14\", \"Position\": 16, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            25: {\"HEX\": \"0x19\", \"Position\": 18, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            27: {\"HEX\": \"0x1B\", \"Position\": 17, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "        }\n",
    "    },\n",
    "    2: {\n",
    "        \"UUID\": \"44332625541024\",\n",
    "        \"SN\": 116,\n",
    "        \"Color\": \"#196ee6\",\n",
    "        \"Sensors\": {\n",
    "            2:  {\"HEX\": \"0x02\", \"Position\": 23, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            3:  {\"HEX\": \"0x03\", \"Position\": 22, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}, \"pv2\": {\"tare\": 0}}},\n",
    "            15: {\"HEX\": \"0x0F\", \"Position\": 21, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            28: {\"HEX\": \"0x1C\", \"Position\": 24, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "        }\n",
    "    },\n",
    "    3: {\n",
    "        \"UUID\": \"39714511164468\",\n",
    "        \"SN\": 115,\n",
    "        \"Color\": \"#8a27a5\",\n",
    "        \"Sensors\": {\n",
    "            1:  {\"HEX\": \"0x01\", \"Position\": 33, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            8:  {\"HEX\": \"0x08\", \"Position\": 38, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            12: {\"HEX\": \"0x0C\", \"Position\": 31, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            13: {\"HEX\": \"0x0D\", \"Position\": 35, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            14: {\"HEX\": \"0x0E\", \"Position\": 34, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            23: {\"HEX\": \"0x17\", \"Position\": 37, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            26: {\"HEX\": \"0x1A\", \"Position\": 36, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            30: {\"HEX\": \"0x1E\", \"Position\": 32, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "        }\n",
    "    },\n",
    "    4: {\n",
    "        \"UUID\": \"207137655863200\",\n",
    "        \"SN\": 114,\n",
    "        \"Color\": \"#c10422\",\n",
    "        \"Sensors\": {\n",
    "            4:  {\"HEX\": \"0x04\", \"Position\": 44, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}, \"pv2\": {\"tare\": 0}}},\n",
    "            22: {\"HEX\": \"0x16\", \"Position\": 43, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            24: {\"HEX\": \"0x18\", \"Position\": 42, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            29: {\"HEX\": \"0x1D\", \"Position\": 41, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            31: {\"HEX\": \"0x1F\", \"Position\": 45, \"Type\": \"TLeaf\", \"Probes\": {\"pv1\": {\"tare\": 0}, \"pv2\": {\"tare\": 0}}},\n",
    "        }\n",
    "    },\n",
    "    5: {\n",
    "        \"UUID\": \"97186493085600\",\n",
    "        \"SN\": 112,\n",
    "        \"Color\": \"#e65d19\",\n",
    "        \"Sensors\": {\n",
    "            5:  {\"HEX\": \"0x05\", \"Position\": 55, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            7:  {\"HEX\": \"0x07\", \"Position\": 53, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            9:  {\"HEX\": \"0x09\", \"Position\": 56, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            16: {\"HEX\": \"0x10\", \"Position\": 54, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            18: {\"HEX\": \"0x12\", \"Position\": 52, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "            21: {\"HEX\": \"0x15\", \"Position\": 51, \"Type\": \"DLeaf\", \"Probes\": {\"pv0\": {\"tare\": 0}, \"pv1\": {\"tare\": 0}}},\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2338de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Define the dates\n",
    "date_from = datetime(2022, 4, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "date_to = datetime(2027, 6, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "# Convert to epoch timestamps\n",
    "epoch_from = int(date_from.timestamp())\n",
    "epoch_to = int(date_to.timestamp())\n",
    "\n",
    "# Print the results\n",
    "print(\"Epoch from:\", epoch_from)\n",
    "print(\"Epoch to:\", epoch_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MongoDB aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"time.server.epoch\": {\n",
    "                \"$gte\": epoch_from,\n",
    "                \"$lte\": epoch_to\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\n",
    "            \"time.server.epoch\": 1\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b26ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch documents\n",
    "documents = list(collection.aggregate(pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage\n",
    "hub_data = {uuid: [] for uuid in [span_info[k][\"UUID\"] for k in span_info]}\n",
    "\n",
    "# Traverse documents\n",
    "for doc in documents:\n",
    "    uuid = str(doc[\"meta\"][\"uuid\"])\n",
    "    timestamp = doc[\"time\"][\"server\"][\"epoch\"]\n",
    "    voltage = doc[\"meta\"][\"power\"][\"battery\"][\"V\"]\n",
    "\n",
    "    # Get list of all spans using this UUID\n",
    "    matching_spans = [k for k, v in span_info.items() if v[\"UUID\"] == uuid]\n",
    "\n",
    "    for span in matching_spans:\n",
    "        record = {\"Time\": timestamp, \"Voltage\": voltage}\n",
    "\n",
    "        # Iterate sensors on that span\n",
    "        for sensor_id, sensor_def in span_info[span][\"Sensors\"].items():\n",
    "            sensor_id_str = str(sensor_id)\n",
    "            sensor_data = doc.get(\"measurements\", {}).get(sensor_id_str)\n",
    "\n",
    "            if not sensor_data:\n",
    "                print(\n",
    "                    f\"⚠ Sensor {sensor_id} missing on span {span} UUID {uuid}\")\n",
    "                continue\n",
    "\n",
    "            sensor_type = sensor_def[\"Type\"]\n",
    "\n",
    "            # Extract requested probes based on sensor type\n",
    "            if sensor_type == \"DLeaf\":\n",
    "                record[f\"D_{sensor_id}_pv0\"] = sensor_data.get(\"pv0\")\n",
    "                record[f\"D_{sensor_id}_pv1\"] = sensor_data.get(\"pv1\")\n",
    "                # For sensors 6 and 10 (span 1), also get pv2\n",
    "                if sensor_id in [3, 4]:\n",
    "                    record[f\"D_{sensor_id}_pv2\"] = sensor_data.get(\"pv2\")\n",
    "\n",
    "            elif sensor_type == \"TLeaf\":\n",
    "                record[f\"T_{sensor_id}_pv1\"] = sensor_data.get(\"pv1\")\n",
    "                record[f\"T_{sensor_id}_pv2\"] = sensor_data.get(\"pv2\")\n",
    "\n",
    "        hub_data[uuid].append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c477355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "df_hub_1 = pd.DataFrame(hub_data[span_info[1][\"UUID\"]])\n",
    "df_hub_2 = pd.DataFrame(hub_data[span_info[2][\"UUID\"]])\n",
    "df_hub_3 = pd.DataFrame(hub_data[span_info[3][\"UUID\"]])\n",
    "df_hub_4 = pd.DataFrame(hub_data[span_info[4][\"UUID\"]])\n",
    "df_hub_5 = pd.DataFrame(hub_data[span_info[5][\"UUID\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hub_1.columns, df_hub_2.columns, df_hub_3.columns, df_hub_4.columns, df_hub_5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Choose one random hub DataFrame\n",
    "df_random = random.choice([\n",
    "    df_hub_1, df_hub_2, df_hub_3, df_hub_4, df_hub_5\n",
    "])\n",
    "\n",
    "# Dropna to ensure we select a column with data\n",
    "non_empty_cols = df_random.drop(\n",
    "    columns=[\"Time\", \"Voltage\"], errors=\"ignore\").dropna(axis=1, how=\"all\").columns\n",
    "if len(non_empty_cols) == 0:\n",
    "    raise ValueError(\"No valid sensor data in selected DataFrame.\")\n",
    "\n",
    "# Pick a random sensor probe column\n",
    "random_col = random.choice(non_empty_cols)\n",
    "\n",
    "# Convert Time if needed\n",
    "df_random[\"Time\"] = pd.to_datetime(df_random[\"Time\"], unit=\"s\", utc=True)\n",
    "\n",
    "# Create Plotly plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_random[\"Time\"],\n",
    "    y=df_random[random_col],\n",
    "    mode=\"lines\",\n",
    "    name=random_col\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Náhodne vybraný priebeh: {random_col}\",\n",
    "    xaxis_title=\"Čas\",\n",
    "    yaxis_title=\"Hodnota\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d89ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(bytes_size):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if bytes_size < 1024:\n",
    "            return f\"{bytes_size:.2f} {unit}\"\n",
    "        bytes_size /= 1024\n",
    "    return f\"{bytes_size:.2f} TB\"\n",
    "\n",
    "# List of all hub DataFrames\n",
    "hub_dfs = {\n",
    "    \"df_hub_1\": df_hub_1,\n",
    "    \"df_hub_2\": df_hub_2,\n",
    "    \"df_hub_3\": df_hub_3,\n",
    "    \"df_hub_4\": df_hub_4,\n",
    "    \"df_hub_5\": df_hub_5,\n",
    "}\n",
    "\n",
    "# Print sizes\n",
    "for name, df in hub_dfs.items():\n",
    "    size_bytes = df.memory_usage(deep=True).sum()\n",
    "    print(f\"{name}: {format_size(size_bytes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_add_datetime(df):\n",
    "    df = df.rename(columns={\"Time\": \"Epochs\"})\n",
    "    df[\"Datetime\"] = pd.to_datetime(\n",
    "        df[\"Epochs\"], unit=\"s\", utc=True).dt.tz_convert(None)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Apply to all three\n",
    "df_hub_1 = rename_and_add_datetime(df_hub_1)\n",
    "df_hub_2 = rename_and_add_datetime(df_hub_2)\n",
    "df_hub_3 = rename_and_add_datetime(df_hub_3)\n",
    "df_hub_4 = rename_and_add_datetime(df_hub_4)\n",
    "df_hub_5 = rename_and_add_datetime(df_hub_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hourly(df):\n",
    "    df = df.set_index(\"Datetime\")  # set datetime index\n",
    "    # resample and keep Datetime\n",
    "    df_resampled = df.resample(\"1h\").mean().reset_index()\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "# Resample all 3\n",
    "df_hub_1 = resample_hourly(df_hub_1)\n",
    "df_hub_2 = resample_hourly(df_hub_2)\n",
    "df_hub_3 = resample_hourly(df_hub_3)\n",
    "df_hub_4 = resample_hourly(df_hub_4)\n",
    "df_hub_5 = resample_hourly(df_hub_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hub_1 = df_hub_1.drop(columns=[\"Epochs\"])\n",
    "df_hub_2 = df_hub_2.drop(columns=[\"Epochs\"])\n",
    "df_hub_3 = df_hub_3.drop(columns=[\"Epochs\"])\n",
    "df_hub_4 = df_hub_4.drop(columns=[\"Epochs\"])\n",
    "df_hub_5 = df_hub_5.drop(columns=[\"Epochs\"])\n",
    "\n",
    "df_hub_1 = df_hub_1.rename(columns={\"Voltage\": \"Volt_H1\"})\n",
    "df_hub_2 = df_hub_2.rename(columns={\"Voltage\": \"Volt_H2\"})\n",
    "df_hub_3 = df_hub_3.rename(columns={\"Voltage\": \"Volt_H3\"})\n",
    "df_hub_4 = df_hub_4.rename(columns={\"Voltage\": \"Volt_H4\"})\n",
    "df_hub_5 = df_hub_5.rename(columns={\"Voltage\": \"Volt_H5\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge by Datetime\n",
    "df_merged = pd.merge(df_hub_1, df_hub_2, on=\"Datetime\", how=\"outer\")\n",
    "df_merged = pd.merge(df_merged, df_hub_3, on=\"Datetime\", how=\"outer\")\n",
    "df_merged = pd.merge(df_merged, df_hub_4, on=\"Datetime\", how=\"outer\")\n",
    "df_merged = pd.merge(df_merged, df_hub_5, on=\"Datetime\", how=\"outer\")\n",
    "\n",
    "# Optional: sort by time\n",
    "df_merged = df_merged.sort_values(\"Datetime\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7551e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hub_1.columns, df_hub_2.columns, df_hub_3.columns, df_hub_4.columns, df_hub_5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean = df_merged.dropna().reset_index(drop=True)\n",
    "df_merged_clean[\"Hour\"] = df_merged_clean[\"Datetime\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc932858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Compute descriptive stats and get standard deviations\n",
    "stds = df_merged_clean.describe().T[\"std\"]\n",
    "\n",
    "# Filter only sensor columns (optional: add \"Hour\" if needed)\n",
    "sensor_stds = stds[stds.index.str.startswith((\"D_\"))]\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "df_std_plot = sensor_stds.reset_index()\n",
    "df_std_plot.columns = [\"Sensor\", \"STD\"]\n",
    "\n",
    "# Sort by STD for readability (optional)\n",
    "df_std_plot = df_std_plot.sort_values(\"STD\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "fig = px.bar(\n",
    "    df_std_plot,\n",
    "    x=\"Sensor\",\n",
    "    y=\"STD\",\n",
    "    title=\"Štandardná odchýlka hodnôt senzorov (STD)\",\n",
    "    labels={\"STD\": \"Štandardná odchýlka\"},\n",
    "    height=600\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c28156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure Datetime is the index\n",
    "import plotly.graph_objects as go\n",
    "df_resampled = df_merged_clean.set_index(\"Datetime\")\n",
    "\n",
    "# Resample to daily mean\n",
    "df_daily = df_resampled.resample(\"1D\").mean().reset_index()\n",
    "\n",
    "# Create Plotly figure\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for col in df_daily.columns:\n",
    "    if col.startswith(\"Volt_H\"):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_daily[\"Datetime\"],\n",
    "            y=df_daily[col],\n",
    "            mode=\"lines+markers\",\n",
    "            name=col\n",
    "        ))\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    title=\"Denný priemer napätia batérie (všetky HUBy)\",\n",
    "    xaxis_title=\"Dátum\",\n",
    "    yaxis_title=\"Napätie [V]\",\n",
    "    template=\"plotly_white\",\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ed583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original rows: {len(df_merged)}\")\n",
    "print(f\"Cleaned rows:  {len(df_merged_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean.iloc[0][\"Datetime\"], df_merged_clean.iloc[-1][\"Datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ef473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only DLeaf sensor columns\n",
    "dleaf_columns = [\n",
    "    col for col in df_merged_clean.columns if col.startswith(\"D_\")]\n",
    "df_d_only = df_merged_clean[dleaf_columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix_d = df_d_only.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11eccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot with seaborn\n",
    "# plt.figure(figsize=(16, 14))\n",
    "# sns.heatmap(\n",
    "#     corr_matrix_d,\n",
    "#     cmap=\"coolwarm\",              # or \"coolwarm\", \"crest\", etc.\n",
    "#     center=0,\n",
    "#     annot=True,\n",
    "#     fmt=\".2f\",\n",
    "#     annot_kws={\"size\": 8},\n",
    "#     cbar_kws={\"label\": \"Correlation\"},\n",
    "#     alpha=0.95\n",
    "# )\n",
    "# plt.title(\"Correlation Matrix of DLeaf Sensors (D_*)\", fontsize=15)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define faulty probes to exclude\n",
    "faulty_probes = {\n",
    "    \"D_23_pv0\", \"D_23_pv1\",  # Hub 3\n",
    "    \"D_19_pv1\",              # Hub 1\n",
    "    \"D_14_pv1\",              # Hub 3\n",
    "    \"D_8_pv0\"                # Hub 3\n",
    "}\n",
    "\n",
    "# Extract DLeaf and specific TLeaf probes\n",
    "dleaf_probes = [col for col in df_merged_clean.columns if col.startswith(\"D_\")]\n",
    "tleaf_probes = [\n",
    "    col for col in df_merged_clean.columns if col.startswith(\"T_31\")]\n",
    "\n",
    "# Build hub groupings from sensor IDs\n",
    "hub_map = {\n",
    "    \"H1\": [col for col in dleaf_probes if any(f\"_{i}_\" in col for i in [6, 10, 11, 17, 19, 20, 25, 27])],\n",
    "    \"H2\": [col for col in dleaf_probes if any(f\"_{i}_\" in col for i in [2, 3, 15, 28])],\n",
    "    \"H3\": [col for col in dleaf_probes if any(f\"_{i}_\" in col for i in [1, 8, 12, 13, 14, 23, 26, 30])],\n",
    "    \"H4\": [col for col in dleaf_probes if any(f\"_{i}_\" in col for i in [4, 22, 24, 29])],\n",
    "    \"H5\": [col for col in dleaf_probes if any(f\"_{i}_\" in col for i in [5, 7, 9, 16, 18, 21])],\n",
    "    \"T31\": tleaf_probes\n",
    "}\n",
    "\n",
    "# Remove faulty probes from each hub group\n",
    "for hub in hub_map:\n",
    "    hub_map[hub] = [probe for probe in hub_map[hub]\n",
    "                    if probe not in faulty_probes]\n",
    "\n",
    "# Build sensor_to_features map, excluding faulty probes\n",
    "sensor_to_features = {}\n",
    "\n",
    "for hub, sensors in hub_map.items():\n",
    "    others = sum([v for k, v in hub_map.items() if k != hub], [])\n",
    "    for s in sensors:\n",
    "        sensor_to_features[s] = [\n",
    "            feat for feat in others if feat not in faulty_probes]\n",
    "\n",
    "# Now `sensor_to_features` is ready and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Filter to DLeaf probes only (reuse sensor_to_features from before)\n",
    "sensor_to_features_dleaf = {\n",
    "    target: feats\n",
    "    for target, feats in sensor_to_features.items()\n",
    "    if target.startswith(\"D_\")\n",
    "}\n",
    "\n",
    "# 2) Prepare containers and define a TimeSeriesSplit\n",
    "linear_models_ts = {}\n",
    "records_linear_ts = []\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 3) Loop over each DLeaf target, perform time-series cross‐validation, then retrain on all data\n",
    "for target, feats in tqdm(sensor_to_features_dleaf.items(), desc=\"TS CV LinearRegression\", unit=\"sensor\"):\n",
    "    # a) Subset rows where neither target nor its features are NaN\n",
    "    cols = feats + [target]\n",
    "    subset = df_merged_clean[cols].dropna()\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    X = subset[feats].values\n",
    "    y = subset[target].values\n",
    "\n",
    "    # b) Cross‐validate using TimeSeriesSplit\n",
    "    rmses, r2s = [], []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = LinearRegression().fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_te)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_te, y_pred)))\n",
    "        r2s.append(r2_score(y_te, y_pred))\n",
    "\n",
    "    # c) Average the fold‐wise metrics\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    avg_r2 = np.mean(r2s)\n",
    "\n",
    "    # d) Retrain on the full available data for deployment\n",
    "    final_model = LinearRegression().fit(X, y)\n",
    "    linear_models_ts[target] = final_model\n",
    "\n",
    "    # e) Record the averaged metrics\n",
    "    records_linear_ts.append({\n",
    "        \"Target\":  target,\n",
    "        \"Avg_RMSE\": avg_rmse,\n",
    "        \"Avg_R²\":   avg_r2\n",
    "    })\n",
    "\n",
    "# 4) Build a summary DataFrame\n",
    "df_linear_ts = pd.DataFrame(records_linear_ts).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990daddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear[\"R²\"].sort_values(ascending=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Restrict to DLeaf probes only (reuse sensor_to_features from before)\n",
    "sensor_to_features_dleaf = {\n",
    "    target: feats\n",
    "    for target, feats in sensor_to_features.items()\n",
    "    if target.startswith(\"D_\")\n",
    "}\n",
    "\n",
    "# 2) Prepare containers and define a TimeSeriesSplit\n",
    "mlp_models_ts = {}\n",
    "records_mlp_ts = []\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 3) MLP hyperparameters (tweak as needed)\n",
    "mlp_config = {\n",
    "    \"hidden_layer_sizes\": (100,),  # one hidden layer of 100 neurons\n",
    "    \"activation\": \"relu\",\n",
    "    \"alpha\": 0.0001,               # L2 penalty\n",
    "    \"learning_rate_init\": 0.001,\n",
    "    \"max_iter\": 1000\n",
    "}\n",
    "\n",
    "# 4) Loop over each DLeaf target\n",
    "for target, feats in tqdm(sensor_to_features_dleaf.items(), desc=\"TS CV MLPRegressor\", unit=\"sensor\"):\n",
    "    # a) Subset rows where neither target nor its features are NaN\n",
    "    cols = feats + [target]\n",
    "    subset = df_merged_clean[cols].dropna()\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    X = subset[feats].values\n",
    "    y = subset[target].values\n",
    "\n",
    "    # b) Cross‐validate using TimeSeriesSplit\n",
    "    rmses, r2s = [], []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "\n",
    "        # i) Build and fit the pipeline on this fold\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"mlp\", MLPRegressor(**mlp_config))\n",
    "        ])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "\n",
    "        # ii) Predict and record metrics\n",
    "        y_pred = pipe.predict(X_te)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_te, y_pred)))\n",
    "        r2s.append(r2_score(y_te, y_pred))\n",
    "\n",
    "    # c) Average the fold‐wise metrics\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    avg_r2 = np.mean(r2s)\n",
    "\n",
    "    # d) Retrain on the full available data for deployment\n",
    "    final_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\", MLPRegressor(**mlp_config))\n",
    "    ])\n",
    "    final_pipe.fit(X, y)\n",
    "    mlp_models_ts[target] = final_pipe\n",
    "\n",
    "    # e) Record the averaged metrics\n",
    "    records_mlp_ts.append({\n",
    "        \"Target\":   target,\n",
    "        \"Avg_RMSE\": avg_rmse,\n",
    "        \"Avg_R²\":   avg_r2\n",
    "    })\n",
    "\n",
    "# 5) Build a summary DataFrame\n",
    "df_mlp_ts = pd.DataFrame(records_mlp_ts).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3af4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.421481 * 25, 0.430359 * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hub_1[\"D_10_pv0\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mlp_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a219f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlp_ts[\"Avg_R²\"].sort_values(ascending=1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bridges_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

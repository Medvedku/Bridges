{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1036eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a19115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved corrected CSVs\n",
    "df_temp_1 = pd.read_csv(\"df_temp_1_corrected.csv\")\n",
    "df_temp_2 = pd.read_csv(\"df_temp_2_corrected.csv\")\n",
    "df_temp_3 = pd.read_csv(\"df_temp_3_corrected.csv\")\n",
    "\n",
    "\n",
    "# Load the saved corrected CSVs\n",
    "df_temp_1_2 = pd.read_csv(\"df_2temp_1_corrected.csv\")\n",
    "df_temp_2_2 = pd.read_csv(\"df_2temp_2_corrected.csv\")\n",
    "df_temp_3_2 = pd.read_csv(\"df_2temp_3_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b641fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with prefix 'dT' from each DataFrame\n",
    "df_temp_1 = df_temp_1.loc[:, ~df_temp_1.columns.str.startswith('dT')]\n",
    "df_temp_2 = df_temp_2.loc[:, ~df_temp_2.columns.str.startswith('dT')]\n",
    "df_temp_3 = df_temp_3.loc[:, ~df_temp_3.columns.str.startswith('dT')]\n",
    "\n",
    "df_temp_1_2 = df_temp_1_2.loc[:, ~df_temp_1_2.columns.str.startswith('dT')]\n",
    "df_temp_2_2 = df_temp_2_2.loc[:, ~df_temp_2_2.columns.str.startswith('dT')]\n",
    "df_temp_3_2 = df_temp_3_2.loc[:, ~df_temp_3_2.columns.str.startswith('dT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.columns, df_temp_2.columns, df_temp_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.columns, df_temp_2.columns, df_temp_3.columns, df_temp_1_2.columns, df_temp_2_2.columns, df_temp_3_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.shape, df_temp_2.shape, df_temp_3.shape, df_temp_1_2.shape, df_temp_2_2.shape, df_temp_3_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b098b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_temp_1, df_temp_2, df_temp_3 (batch 1)\n",
    "# and df_temp_1_2, df_temp_2_2, df_temp_3_2 (batch 2) are already loaded\n",
    "\n",
    "# For df_temp_1:\n",
    "# 1) Find the maximum ‚ÄúTime‚Äù value in the first batch\n",
    "max_time_1 = df_temp_1[\"Time\"].max()\n",
    "\n",
    "# 2) Select only those rows from batch 2 whose Time is strictly greater than max_time_1\n",
    "new_rows_1 = df_temp_1_2[df_temp_1_2[\"Time\"] > max_time_1].copy()\n",
    "\n",
    "# 3) Concatenate and reset the index (ignore_index=True to get a new contiguous index)\n",
    "df_temp_1 = pd.concat([df_temp_1, new_rows_1], ignore_index=True)\n",
    "\n",
    "# For df_temp_2:\n",
    "max_time_2 = df_temp_2[\"Time\"].max()\n",
    "new_rows_2 = df_temp_2_2[df_temp_2_2[\"Time\"] > max_time_2].copy()\n",
    "df_temp_2 = pd.concat([df_temp_2, new_rows_2], ignore_index=True)\n",
    "\n",
    "# For df_temp_3:\n",
    "max_time_3 = df_temp_3[\"Time\"].max()\n",
    "new_rows_3 = df_temp_3_2[df_temp_3_2[\"Time\"] > max_time_3].copy()\n",
    "df_temp_3 = pd.concat([df_temp_3, new_rows_3], ignore_index=True)\n",
    "\n",
    "# After running this, df_temp_1, df_temp_2, df_temp_3 each include only the ‚Äúnew‚Äù rows\n",
    "# from the second‚Äêbatch DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to apply to all DataFrames\n",
    "def rename_and_add_datetime(df):\n",
    "    df = df.rename(columns={\"Time\": \"Epochs\"})\n",
    "    df[\"Datetime\"] = pd.to_datetime(df[\"Epochs\"], unit=\"s\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply to all three\n",
    "df_temp_1 = rename_and_add_datetime(df_temp_1)\n",
    "df_temp_2 = rename_and_add_datetime(df_temp_2)\n",
    "df_temp_3 = rename_and_add_datetime(df_temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hourly(df):\n",
    "    df = df.set_index(\"Datetime\")  # set datetime index\n",
    "    # resample and keep Datetime\n",
    "    df_resampled = df.resample(\"1h\").mean().reset_index()\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "# Resample all 3\n",
    "df_temp_1 = resample_hourly(df_temp_1)\n",
    "df_temp_2 = resample_hourly(df_temp_2)\n",
    "df_temp_3 = resample_hourly(df_temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1 = df_temp_1.drop(columns=[\"Epochs\"])\n",
    "df_temp_2 = df_temp_2.drop(columns=[\"Epochs\"])\n",
    "df_temp_3 = df_temp_3.drop(columns=[\"Epochs\"])\n",
    "\n",
    "df_temp_1 = df_temp_1.rename(columns={\"Voltage\": \"Volt_H1\"})\n",
    "df_temp_2 = df_temp_2.rename(columns={\"Voltage\": \"Volt_H2\"})\n",
    "df_temp_3 = df_temp_3.rename(columns={\"Voltage\": \"Volt_H3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727eedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.columns, df_temp_2.columns, df_temp_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge by Datetime\n",
    "df_merged = pd.merge(df_temp_1, df_temp_2, on=\"Datetime\", how=\"outer\")\n",
    "df_merged = pd.merge(df_merged, df_temp_3, on=\"Datetime\", how=\"outer\")\n",
    "\n",
    "# Optional: sort by time\n",
    "df_merged = df_merged.sort_values(\"Datetime\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean = df_merged.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original rows: {len(df_merged)}\")\n",
    "print(f\"Cleaned rows:  {len(df_merged_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32816b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean[\"Hour\"] = df_merged_clean[\"Datetime\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83286b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff856ae3",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b458996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only temperature sensor columns\n",
    "temp_columns = [col for col in df_merged_clean.columns if col.startswith(\"T_\")]\n",
    "df_t_only = df_merged_clean[temp_columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix_t = df_t_only.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with opacity and annotations\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    corr_matrix_t,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=True,                   # Show values\n",
    "    fmt=\".2f\",                    # Format to 2 decimal places\n",
    "    annot_kws={\"size\": 8},\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    "    alpha=0.99                     # Set opacity\n",
    ")\n",
    "plt.title(\"Correlation Matrix of Temperature Sensors (T_*)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c8640",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae148ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Filter T_ columns\n",
    "temp_columns = [col for col in df_merged_clean.columns if col.startswith(\"T_\")]\n",
    "df_t_only = df_merged_clean[temp_columns]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_t_only.corr()\n",
    "\n",
    "# Extract upper triangle without diagonal\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Flatten and drop NaNs\n",
    "correlations = upper_tri.unstack().dropna()\n",
    "\n",
    "# Describe the correlation values\n",
    "print(\"üìä Descriptive statistics for pairwise T-sensor correlations:\\n\")\n",
    "print(correlations.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1238df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Initialization with predefined MultiIndex columns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Copy your data\n",
    "df = df_merged_clean.copy()\n",
    "\n",
    "# Define hub groups\n",
    "hub_1 = ['T_15_pv1', 'T_15_pv2', 'T_17_pv1', 'T_17_pv2',\n",
    "         'T_19_pv1', 'T_19_pv2', 'T_14_pv1', 'T_14_pv2']\n",
    "hub_2 = ['T_16_pv1', 'T_16_pv2', 'T_13_pv1', 'T_13_pv2',\n",
    "         'T_18_pv1', 'T_18_pv2', 'T_20_pv1', 'T_20_pv2']\n",
    "hub_3 = ['T_11_pv1', 'T_11_pv2', 'T_12_pv1',\n",
    "         'T_12_pv2', 'T_27_pv1', 'T_27_pv2']\n",
    "\n",
    "sensor_to_features = {}\n",
    "for s in hub_1:\n",
    "    sensor_to_features[s] = hub_2 + hub_3\n",
    "for s in hub_2:\n",
    "    sensor_to_features[s] = hub_1 + hub_3\n",
    "for s in hub_3:\n",
    "    sensor_to_features[s] = hub_1 + hub_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62c107",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efda13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Load the pickled models back into Python\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Same filename used when saving\n",
    "model_filename = \"linear_models.pkl\"\n",
    "\n",
    "# Load the dictionary of models\n",
    "with open(model_filename, \"rb\") as f:\n",
    "    loaded_linear_models = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(loaded_linear_models)} models from {model_filename}\")\n",
    "\n",
    "# Example: use one loaded model to predict\n",
    "# sensor = list(loaded_linear_models.keys())[0]\n",
    "# preds = loaded_linear_models[sensor].predict(df_merged_clean[sensor_to_features[sensor]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Prepare a list to collect metrics from loaded models\n",
    "records_linear = []\n",
    "\n",
    "# 2) Iterate over loaded models and evaluate on 20% hold-out\n",
    "for target, model in tqdm(loaded_linear_models.items(), desc=\"LinearRegression (Loaded)\", unit=\"sensor\"):\n",
    "    # a) Extract features and target\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # b) Perform same 80/20 time-ordered split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # c) Predict on test split\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    # d) Record RMSE / R¬≤\n",
    "    records_linear.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": {},  # still none for LinearRegression\n",
    "        \"RMSE\":       np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\":         r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "# 3) Assemble into DataFrame\n",
    "df_linear = pd.DataFrame(records_linear).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_linear[[\"RMSE\", \"R¬≤\"]]), df_linear[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202b162",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load Ridge models from the saved pickle file\n",
    "with open(\"ridge_models.pkl\", \"rb\") as f:\n",
    "    loaded_ridge_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Output list for metrics\n",
    "records = []\n",
    "\n",
    "# Recompute metrics using same test split strategy\n",
    "for target, model in loaded_ridge_models.items():\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # Split exactly like original: 80% train, 20% test (time-order preserved)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    records.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\"alpha\": model.alpha},  # optional\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\": r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_ridge = pd.DataFrame(records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93583bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_ridge[[\"RMSE\", \"R¬≤\"]]), df_ridge[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb0b53",
   "metadata": {},
   "source": [
    "### Ridge + Poly2 + TSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76290abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load models from .pkl file\n",
    "with open(\"ridge_poly2_tscv_models.pkl\", \"rb\") as f:\n",
    "    loaded_ridge_poly2_tscv_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Containers\n",
    "records = []\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 2) Evaluate loaded Ridge+Poly2 models using TSCV\n",
    "for target, model in tqdm(loaded_ridge_poly2_tscv_models.items(), desc=\"Recompute Ridge+Poly2+TSCV\", unit=\"sensor\"):\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats].values, df[target].values\n",
    "\n",
    "    y_preds = np.full_like(y, np.nan, dtype=float)\n",
    "\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr = y[train_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_preds[test_idx] = model.predict(X_te)\n",
    "\n",
    "    valid_mask = ~np.isnan(y_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y[valid_mask], y_preds[valid_mask]))\n",
    "    r2 = r2_score(y[valid_mask], y_preds[valid_mask])\n",
    "\n",
    "    records.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": {\"alpha\": model.named_steps[\"ridge\"].alpha},\n",
    "        \"RMSE\":       rmse,\n",
    "        \"R¬≤\":         r2\n",
    "    })\n",
    "\n",
    "# 3) Wrap into a DataFrame\n",
    "df_ridge_poly2_tscv = pd.DataFrame(records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_ridge_poly2_tscv[[\"RMSE\", \"R¬≤\"]]\n",
    " ), df_ridge_poly2_tscv[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c5f76",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a25d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load models\n",
    "knn_model_filename = \"models_knn.pkl\"\n",
    "\n",
    "with open(knn_model_filename, \"rb\") as f:\n",
    "    loaded_models_knn = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(loaded_models_knn)} KNN models from '{knn_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluation container\n",
    "recomputed_knn_records = []\n",
    "\n",
    "for target, model in loaded_models_knn.items():\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # Consistent 80/20 split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    # Store metrics\n",
    "    recomputed_knn_records.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\n",
    "            \"k\": model.named_steps[\"knn\"].n_neighbors,\n",
    "            \"weights\": model.named_steps[\"knn\"].weights\n",
    "        },\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\": r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "# Results DataFrame\n",
    "df_knn = pd.DataFrame(recomputed_knn_records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_knn[[\"RMSE\", \"R¬≤\"]]\n",
    " ), df_knn[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0459b",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Unpickle the RandomForest models in a new notebook\n",
    "import pickle\n",
    "\n",
    "# Load models\n",
    "rf_model_filename = \"models_rf.pkl\"\n",
    "\n",
    "with open(rf_model_filename, \"rb\") as f:\n",
    "    loaded_models_rf = pickle.load(f)\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(loaded_models_rf)} RandomForest models from '{rf_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e099b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Re-evaluate loaded RF models on 80/20 time-ordered split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Container for recomputed metrics\n",
    "recomputed_rf_records = []\n",
    "\n",
    "for target, model in loaded_models_rf.items():\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # Same 80/20 time-ordered split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Predict on test split\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    # Record RMSE and R¬≤\n",
    "    recomputed_rf_records.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\n",
    "            \"n_estimators\": model.n_estimators,\n",
    "            \"max_depth\": model.max_depth,\n",
    "            \"max_features\": model.max_features\n",
    "        },\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\": r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "# Build the final DataFrame\n",
    "df_rf = pd.DataFrame(recomputed_rf_records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_rf[[\"RMSE\", \"R¬≤\"]]\n",
    " ), df_rf[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae725c27",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the pickled MLP models in a new notebook\n",
    "\n",
    "import pickle\n",
    "\n",
    "mlp_model_filename = \"mlp_models.pkl\"\n",
    "with open(mlp_model_filename, \"rb\") as f:\n",
    "    loaded_mlp_models = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(loaded_mlp_models)} MLP models from '{mlp_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Recompute metrics (RMSE and R¬≤) on the same 80/20 split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "records_mlp_recomputed = []\n",
    "\n",
    "for target, model in loaded_mlp_models.items():\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # Same 80/20 time-ordered split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    records_mlp_recomputed.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": model.get_params(),\n",
    "        \"RMSE\":       np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\":         r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "df_mlp = pd.DataFrame(records_mlp_recomputed).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_mlp[[\"RMSE\", \"R¬≤\"]]\n",
    " ), df_mlp[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the pickled GradientBoosting models in a new notebook\n",
    "\n",
    "import pickle\n",
    "\n",
    "gb_model_filename = \"models_gb.pkl\"\n",
    "with open(gb_model_filename, \"rb\") as f:\n",
    "    loaded_models_gb = pickle.load(f)\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(loaded_models_gb)} GradientBoosting models from '{gb_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Recompute metrics (RMSE and R¬≤) on the same 80/20 split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "recomputed_gb_records = []\n",
    "\n",
    "for target, model in loaded_models_gb.items():\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # Same 80/20 time‚Äêordered split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    recomputed_gb_records.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": {\n",
    "            \"n_estimators\": model.named_steps[\"gbrt\"].n_estimators,\n",
    "            \"max_depth\":    model.named_steps[\"gbrt\"].max_depth,\n",
    "            \"learning_rate\": model.named_steps[\"gbrt\"].learning_rate\n",
    "        },\n",
    "        \"RMSE\":       np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\":         r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "df_gb = pd.DataFrame(recomputed_gb_records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_gb[[\"RMSE\", \"R¬≤\"]]\n",
    " ), df_gb[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d28da",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d33e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the pickled SVR models in a new notebook\n",
    "\n",
    "import pickle\n",
    "\n",
    "svr_model_filename = \"models_svr.pkl\"\n",
    "with open(svr_model_filename, \"rb\") as f:\n",
    "    loaded_models_svr = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(loaded_models_svr)} SVR models from '{svr_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3afa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Recompute metrics (RMSE and R¬≤) on the same 80/20 split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "recomputed_svr_records = []\n",
    "\n",
    "for target, model in loaded_models_svr.items():\n",
    "    feats = sensor_to_features[target]\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # Same 80/20 time‚Äêordered split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    svr_step = model.named_steps[\"svr\"]\n",
    "    recomputed_svr_records.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": {\n",
    "            \"C\":       svr_step.C,\n",
    "            \"epsilon\": svr_step.epsilon,\n",
    "            \"gamma\":   svr_step.gamma\n",
    "        },\n",
    "        \"RMSE\":       np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R¬≤\":         r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "df_svr = pd.DataFrame(recomputed_svr_records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ca426",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_svr[[\"RMSE\", \"R¬≤\"]]\n",
    " ), df_svr[[\"RMSE\", \"R¬≤\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efeaec",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select only the RMSE and R¬≤ columns from each results DataFrame\n",
    "df_linear_metrics = df_linear[[\"RMSE\", \"R¬≤\"]]\n",
    "df_ridge_metrics = df_ridge[[\"RMSE\", \"R¬≤\"]]\n",
    "df_ridge_poly2_metrics = df_ridge_poly2_tscv[[\"RMSE\", \"R¬≤\"]]\n",
    "df_knn_metrics = df_knn[[\"RMSE\", \"R¬≤\"]]\n",
    "df_rf_metrics = df_rf[[\"RMSE\", \"R¬≤\"]]\n",
    "df_mlp_metrics = df_mlp[[\"RMSE\", \"R¬≤\"]]\n",
    "df_gb_metrics = df_gb[[\"RMSE\", \"R¬≤\"]]\n",
    "df_svr_metrics = df_svr[[\"RMSE\", \"R¬≤\"]]\n",
    "\n",
    "# Concatenate them side‚Äêby‚Äêside with a nested (MultiIndex) column header\n",
    "df_comparison = pd.concat(\n",
    "    {\n",
    "        \"Linear\":     df_linear_metrics,\n",
    "        \"Ridge\":      df_ridge_metrics,\n",
    "        \"RidgePoly2\": df_ridge_poly2_metrics,\n",
    "        \"KNN\":        df_knn_metrics,\n",
    "        \"Random Forrest\":           df_rf_metrics,\n",
    "        \"MLP\":        df_mlp_metrics,\n",
    "        \"GradBoostReg\": df_gb_metrics, \n",
    "        \"SVR\": df_svr_metrics\n",
    "    },\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba405d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_comparison.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) List of model names in df_comparison\n",
    "model_names = df_comparison.columns.levels[0].tolist()\n",
    "\n",
    "# 2) For each sensor, find which model has the minimum RMSE\n",
    "best_by_rmse = {}\n",
    "for sensor, row in df_comparison.iterrows():\n",
    "    # Extract RMSE values for all models\n",
    "    rmse_vals = {model: row[(model, \"RMSE\")] for model in model_names}\n",
    "    # Pick the model with the smallest RMSE\n",
    "    best_model = min(rmse_vals, key=rmse_vals.get)\n",
    "    best_by_rmse[sensor] = best_model\n",
    "\n",
    "# 3) Build a small DataFrame\n",
    "df_best_models = (\n",
    "    pd.DataFrame.from_dict(best_by_rmse, orient=\"index\", columns=[\"BestModel\"])\n",
    "      .sort_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_best_models.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e253a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) List of model names from df_comparison\n",
    "model_names = df_comparison.columns.levels[0].tolist()\n",
    "\n",
    "# 2) Compute ranks for each sensor based on RMSE\n",
    "ranked_models = {}\n",
    "\n",
    "for sensor, row in df_comparison.iterrows():\n",
    "    rmse_vals = {model: row[(model, \"RMSE\")] for model in model_names}\n",
    "\n",
    "    # Sort by RMSE ascending (lower is better)\n",
    "    sorted_models = sorted(rmse_vals.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Save with explicit ranks\n",
    "    ranked_models[sensor] = {\n",
    "        f\"Rank_{i+1}\": model for i, (model, _) in enumerate(sorted_models)\n",
    "    }\n",
    "\n",
    "# 3) Create final DataFrame with ranks\n",
    "df_model_ranks = pd.DataFrame.from_dict(\n",
    "    ranked_models, orient=\"index\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aeb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df_model_ranks contains columns like Rank_1, Rank_2, ..., Rank_N\n",
    "rank_columns = df_model_ranks.columns\n",
    "\n",
    "# Apply value_counts per column and combine into a single DataFrame\n",
    "rank_summary = pd.concat(\n",
    "    [df_model_ranks[col].value_counts() for col in rank_columns],\n",
    "    axis=1\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "# Rename columns for clarity (optional, already Rank_1 etc.)\n",
    "rank_summary.columns = rank_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Non-sensor metadata columns\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "base_df = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "# Template to reduce redundancy\n",
    "\n",
    "\n",
    "def generate_predictions(model_dict, name):\n",
    "    df_pred = base_df.copy()\n",
    "    for s, model in tqdm(model_dict.items(), desc=f\"Predicting {name}\", unit=\"sensor\"):\n",
    "        X_full = df_merged_clean[sensor_to_features[s]]\n",
    "        df_pred[s] = model.predict(X_full)\n",
    "    df_pred = df_pred[df_merged_clean.columns].copy()\n",
    "    return df_pred\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "df_linear_merged_clean = generate_predictions(loaded_linear_models, \"Linear\")\n",
    "df_ridge_merged_clean = generate_predictions(loaded_ridge_models, \"Ridge\")\n",
    "df_ridge_poly2_tscv_merged_clean = generate_predictions(\n",
    "    loaded_ridge_poly2_tscv_models, \"RidgePoly2\")\n",
    "df_knn_merged_clean = generate_predictions(loaded_models_knn, \"KNN\")\n",
    "df_rf_merged_clean = generate_predictions(loaded_models_rf, \"RandomForest\")\n",
    "df_mlp_merged_clean = generate_predictions(loaded_mlp_models, \"MLP\")\n",
    "df_gb_merged_clean = generate_predictions(loaded_models_gb, \"GradientBoosting\")\n",
    "df_svr_merged_clean = generate_predictions(loaded_models_svr, \"SVR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6832d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_scheme = {\n",
    "    \"Measured\": \"#000000\",\n",
    "    \"Linear\": \"#1f77b4\",\n",
    "    \"Ridge\": \"#2ca02c\",\n",
    "    \"RidgePoly2\": \"#ff7f0e\",\n",
    "    \"KNN\": \"#9467bd\",\n",
    "    \"RandomForest\": \"#17becf\",\n",
    "    \"MLP\": \"#e377c2\",\n",
    "    \"GradientBoosting\": \"#d62728\",\n",
    "    \"SVR\": \"#8c564b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43685022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Choose the sensor you want to plot, e.g.:\n",
    "sensor = \"T_14_pv1\"\n",
    "\n",
    "# Create a new figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add measured values\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_merged_clean[\"Datetime\"],\n",
    "    y=df_merged_clean[sensor],\n",
    "    mode=\"markers\",\n",
    "    name=\"Measured\",\n",
    "    marker=dict(size=6, color=color_scheme[\"Measured\"], opacity=0.6)\n",
    "))\n",
    "\n",
    "# Helper function to add prediction line\n",
    "\n",
    "\n",
    "def add_prediction_trace(df_pred, model_name):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_pred[\"Datetime\"],\n",
    "        y=df_pred[sensor],\n",
    "        mode=\"lines\",\n",
    "        name=model_name,\n",
    "        line=dict(width=2, color=color_scheme[model_name])\n",
    "    ))\n",
    "\n",
    "\n",
    "# Add all models\n",
    "add_prediction_trace(df_linear_merged_clean, \"Linear\")\n",
    "add_prediction_trace(df_ridge_merged_clean, \"Ridge\")\n",
    "add_prediction_trace(df_ridge_poly2_tscv_merged_clean, \"RidgePoly2\")\n",
    "add_prediction_trace(df_knn_merged_clean, \"KNN\")\n",
    "add_prediction_trace(df_rf_merged_clean, \"RandomForest\")\n",
    "add_prediction_trace(df_mlp_merged_clean, \"MLP\")\n",
    "add_prediction_trace(df_gb_merged_clean, \"GradientBoosting\")\n",
    "add_prediction_trace(df_svr_merged_clean, \"SVR\")\n",
    "\n",
    "# Final layout\n",
    "fig.update_layout(\n",
    "    title=f\"Predictions vs Measured: {sensor}\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"Temperature [¬∞C]\",\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\",\n",
    "                y=1.02, xanchor=\"right\", x=1),\n",
    "    width=1100,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Matplotlib version of plot_and_save\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "\n",
    "def plot_and_save_matplotlib(sensor, model_name, df_measured, df_predicted, df_stats, out_dir=\"images\"):\n",
    "    \"\"\"\n",
    "    Creates a scatter of measured vs predicted for a given sensor and model using matplotlib,\n",
    "    annotates RMSE/R¬≤, and saves the figure as a PNG.\n",
    "\n",
    "    Parameters:\n",
    "    - sensor:        String (e.g. \"T_14_pv1\")\n",
    "    - model_name:    String matching a key in color_scheme (e.g. \"Ridge\")\n",
    "    - df_measured:   DataFrame with actual sensor values (must contain the sensor column)\n",
    "    - df_predicted:  DataFrame with predictions from this model (same sensor columns)\n",
    "    - df_stats:      DataFrame indexed by sensor, with columns [\"RMSE\",\"R¬≤\"] for this model\n",
    "    - out_dir:       Subfolder where images are saved (default \"images\")\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    measured = df_measured[sensor].values\n",
    "    predicted = df_predicted[sensor].values\n",
    "\n",
    "    rmse_val = df_stats.loc[sensor, \"RMSE\"]\n",
    "    r2_val = df_stats.loc[sensor, \"R¬≤\"]\n",
    "    annotation_text = f\"Model: {model_name}\\nR¬≤ = {r2_val:.3f}\\nRMSE = {rmse_val:.3f}\"\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(4.0, 4.0))\n",
    "\n",
    "    # Scatter: measured vs predicted\n",
    "    ax.scatter(\n",
    "        measured,\n",
    "        predicted,\n",
    "        color=color_scheme[model_name],\n",
    "        s=16,\n",
    "        alpha=0.6,\n",
    "        label=\"Data\"\n",
    "    )\n",
    "\n",
    "    # y = x reference line\n",
    "    min_val = min(measured.min(), predicted.min())\n",
    "    max_val = max(measured.max(), predicted.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val],\n",
    "            color=\"gray\", linestyle=\"--\", label=\"y = x\")\n",
    "\n",
    "    # Annotation of RMSE and R¬≤\n",
    "    ax.text(\n",
    "        0.95, 0.05,\n",
    "        annotation_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\",\n",
    "                  edgecolor=\"black\", alpha=0.8)\n",
    "    )\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_title(f\"{model_name}: Comparsion for {sensor}\")\n",
    "    ax.set_xlabel(\"Measured\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "    # Save as PNG\n",
    "    outfile = os.path.join(out_dir, f\"{model_name}_{sensor}.png\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outfile, dpi=150)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec122923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Single-call test with matplotlib version (for example)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# Choose a sensor and model to test\n",
    "test_sensor = \"T_14_pv1\"\n",
    "test_model = \"Ridge\"\n",
    "\n",
    "# Call the matplotlib-based function once\n",
    "plot_and_save_matplotlib(\n",
    "    sensor=test_sensor,\n",
    "    model_name=test_model,\n",
    "    df_measured=df_merged_clean,\n",
    "    df_predicted=df_ridge_merged_clean,\n",
    "    df_stats=df_ridge,\n",
    "    out_dir=\"images\"\n",
    ")\n",
    "\n",
    "print(f\"Saved image for {test_model}_{test_sensor}.png to './images'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "   \"Linear\":           (df_linear_merged_clean, df_linear),\n",
    "   \"Ridge\":            (df_ridge_merged_clean, df_ridge),\n",
    "   \"RidgePoly2\":       (df_ridge_poly2_tscv_merged_clean, df_ridge_poly2_tscv),\n",
    "   \"KNN\":              (df_knn_merged_clean, df_knn),\n",
    "   \"RandomForest\":     (df_rf_merged_clean, df_rf),\n",
    "   \"MLP\":              (df_mlp_merged_clean, df_mlp),\n",
    "   \"GradientBoosting\": (df_gb_merged_clean, df_gb),\n",
    "   \"SVR\":              (df_svr_merged_clean, df_svr)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Map each model name to its merged‚Äêpredictions DataFrame and stats DataFrame\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "model_pred_dfs = {\n",
    "    \"Linear\":           df_linear_merged_clean,\n",
    "    \"Ridge\":            df_ridge_merged_clean,\n",
    "    \"RidgePoly2\":       df_ridge_poly2_tscv_merged_clean,\n",
    "    \"KNN\":              df_knn_merged_clean,\n",
    "    \"RandomForest\":     df_rf_merged_clean,\n",
    "    \"MLP\":              df_mlp_merged_clean,\n",
    "    \"GradientBoosting\": df_gb_merged_clean,\n",
    "    \"SVR\":              df_svr_merged_clean\n",
    "}\n",
    "\n",
    "model_stats_dfs = {\n",
    "    \"Linear\":           df_linear,\n",
    "    \"Ridge\":            df_ridge,\n",
    "    \"RidgePoly2\":       df_ridge_poly2_tscv,\n",
    "    \"KNN\":              df_knn,\n",
    "    \"RandomForest\":     df_rf,\n",
    "    \"MLP\":              df_mlp,\n",
    "    \"GradientBoosting\": df_gb,\n",
    "    \"SVR\":              df_svr\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Ensure output folder exists once\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Loop over sensors and plot only their best model\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "for sensor, best_model in df_best_models[\"BestModel\"].items():\n",
    "    df_predicted = model_pred_dfs[best_model]\n",
    "    df_stats = model_stats_dfs[best_model]\n",
    "\n",
    "    # Call the matplotlib save function\n",
    "    plot_and_save_matplotlib(\n",
    "        sensor=sensor,\n",
    "        model_name=best_model,\n",
    "        df_measured=df_merged_clean,\n",
    "        df_predicted=df_predicted,\n",
    "        df_stats=df_stats,\n",
    "        out_dir=\"images\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for model_name, (df_predicted, df_stats) in model_data.items():\n",
    "#     for sensor in df_stats.index:\n",
    "#         plot_and_save_matplotlib(\n",
    "#             sensor=sensor,\n",
    "#             model_name=model_name,\n",
    "#             df_measured=df_merged_clean,\n",
    "#             df_predicted=df_predicted,\n",
    "#             df_stats=df_stats,\n",
    "#             out_dir=\"images\"\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bridges_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

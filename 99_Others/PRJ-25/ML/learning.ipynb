{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a19115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved corrected CSVs\n",
    "df_temp_1 = pd.read_csv(\"df_temp_1_corrected.csv\")\n",
    "df_temp_2 = pd.read_csv(\"df_temp_2_corrected.csv\")\n",
    "df_temp_3 = pd.read_csv(\"df_temp_3_corrected.csv\")\n",
    "\n",
    "\n",
    "# Load the saved corrected CSVs\n",
    "df_temp_1_2 = pd.read_csv(\"df_2temp_1_corrected.csv\")\n",
    "df_temp_2_2 = pd.read_csv(\"df_2temp_2_corrected.csv\")\n",
    "df_temp_3_2 = pd.read_csv(\"df_2temp_3_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b641fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with prefix 'dT' from each DataFrame\n",
    "df_temp_1 = df_temp_1.loc[:, ~df_temp_1.columns.str.startswith('dT')]\n",
    "df_temp_2 = df_temp_2.loc[:, ~df_temp_2.columns.str.startswith('dT')]\n",
    "df_temp_3 = df_temp_3.loc[:, ~df_temp_3.columns.str.startswith('dT')]\n",
    "\n",
    "df_temp_1_2 = df_temp_1_2.loc[:, ~df_temp_1_2.columns.str.startswith('dT')]\n",
    "df_temp_2_2 = df_temp_2_2.loc[:, ~df_temp_2_2.columns.str.startswith('dT')]\n",
    "df_temp_3_2 = df_temp_3_2.loc[:, ~df_temp_3_2.columns.str.startswith('dT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.columns, df_temp_2.columns, df_temp_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.columns, df_temp_2.columns, df_temp_3.columns, df_temp_1_2.columns, df_temp_2_2.columns, df_temp_3_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.shape, df_temp_2.shape, df_temp_3.shape, df_temp_1_2.shape, df_temp_2_2.shape, df_temp_3_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b098b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_temp_1, df_temp_2, df_temp_3 (batch 1)\n",
    "# and df_temp_1_2, df_temp_2_2, df_temp_3_2 (batch 2) are already loaded\n",
    "\n",
    "# For df_temp_1:\n",
    "# 1) Find the maximum “Time” value in the first batch\n",
    "max_time_1 = df_temp_1[\"Time\"].max()\n",
    "\n",
    "# 2) Select only those rows from batch 2 whose Time is strictly greater than max_time_1\n",
    "new_rows_1 = df_temp_1_2[df_temp_1_2[\"Time\"] > max_time_1].copy()\n",
    "\n",
    "# 3) Concatenate and reset the index (ignore_index=True to get a new contiguous index)\n",
    "df_temp_1 = pd.concat([df_temp_1, new_rows_1], ignore_index=True)\n",
    "\n",
    "# For df_temp_2:\n",
    "max_time_2 = df_temp_2[\"Time\"].max()\n",
    "new_rows_2 = df_temp_2_2[df_temp_2_2[\"Time\"] > max_time_2].copy()\n",
    "df_temp_2 = pd.concat([df_temp_2, new_rows_2], ignore_index=True)\n",
    "\n",
    "# For df_temp_3:\n",
    "max_time_3 = df_temp_3[\"Time\"].max()\n",
    "new_rows_3 = df_temp_3_2[df_temp_3_2[\"Time\"] > max_time_3].copy()\n",
    "df_temp_3 = pd.concat([df_temp_3, new_rows_3], ignore_index=True)\n",
    "\n",
    "# After running this, df_temp_1, df_temp_2, df_temp_3 each include only the “new” rows\n",
    "# from the second‐batch DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to apply to all DataFrames\n",
    "def rename_and_add_datetime(df):\n",
    "    df = df.rename(columns={\"Time\": \"Epochs\"})\n",
    "    df[\"Datetime\"] = pd.to_datetime(df[\"Epochs\"], unit=\"s\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply to all three\n",
    "df_temp_1 = rename_and_add_datetime(df_temp_1)\n",
    "df_temp_2 = rename_and_add_datetime(df_temp_2)\n",
    "df_temp_3 = rename_and_add_datetime(df_temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hourly(df):\n",
    "    df = df.set_index(\"Datetime\")  # set datetime index\n",
    "    # resample and keep Datetime\n",
    "    df_resampled = df.resample(\"1h\").mean().reset_index()\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "# Resample all 3\n",
    "df_temp_1 = resample_hourly(df_temp_1)\n",
    "df_temp_2 = resample_hourly(df_temp_2)\n",
    "df_temp_3 = resample_hourly(df_temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1 = df_temp_1.drop(columns=[\"Epochs\"])\n",
    "df_temp_2 = df_temp_2.drop(columns=[\"Epochs\"])\n",
    "df_temp_3 = df_temp_3.drop(columns=[\"Epochs\"])\n",
    "\n",
    "df_temp_1 = df_temp_1.rename(columns={\"Voltage\": \"Volt_H1\"})\n",
    "df_temp_2 = df_temp_2.rename(columns={\"Voltage\": \"Volt_H2\"})\n",
    "df_temp_3 = df_temp_3.rename(columns={\"Voltage\": \"Volt_H3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727eedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1.columns, df_temp_2.columns, df_temp_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge by Datetime\n",
    "df_merged = pd.merge(df_temp_1, df_temp_2, on=\"Datetime\", how=\"outer\")\n",
    "df_merged = pd.merge(df_merged, df_temp_3, on=\"Datetime\", how=\"outer\")\n",
    "\n",
    "# Optional: sort by time\n",
    "df_merged = df_merged.sort_values(\"Datetime\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nans = df_merged[df_merged.isna().any(axis=1)]\n",
    "print(rows_with_nans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rows with any NaNs: {rows_with_nans.shape[0]} of {df_merged.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Hub list\n",
    "hubs = [('Hub 1', df_temp_1), ('Hub 2', df_temp_2), ('Hub 3', df_temp_3)]\n",
    "\n",
    "# Add all T_ series to the figure using human-readable datetime\n",
    "for hub_name, df in hubs:\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"T_\") and not col.startswith(\"dT_\"):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['Datetime'],  # ✅ Use datetime here\n",
    "                y=df[col],\n",
    "                mode='lines',\n",
    "                name=f\"{hub_name} - {col}\"\n",
    "            ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"All Temperature Series by Datetime\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"Temperature (°C)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean = df_merged.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original rows: {len(df_merged)}\")\n",
    "print(f\"Cleaned rows:  {len(df_merged_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32816b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean[\"Hour\"] = df_merged_clean[\"Datetime\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83286b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb93223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff856ae3",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b458996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only temperature sensor columns\n",
    "temp_columns = [col for col in df_merged_clean.columns if col.startswith(\"T_\")]\n",
    "df_t_only = df_merged_clean[temp_columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix_t = df_t_only.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c65d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with opacity and annotations\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    corr_matrix_t,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=True,                   # Show values\n",
    "    fmt=\".2f\",                    # Format to 2 decimal places\n",
    "    annot_kws={\"size\": 8},\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    "    alpha=0.99                     # Set opacity\n",
    ")\n",
    "plt.title(\"Correlation Matrix of Temperature Sensors (T_*)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c8640",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae148ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Filter T_ columns\n",
    "temp_columns = [col for col in df_merged_clean.columns if col.startswith(\"T_\")]\n",
    "df_t_only = df_merged_clean[temp_columns]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_t_only.corr()\n",
    "\n",
    "# Extract upper triangle without diagonal\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Flatten and drop NaNs\n",
    "correlations = upper_tri.unstack().dropna()\n",
    "\n",
    "# Describe the correlation values\n",
    "print(\"📊 Descriptive statistics for pairwise T-sensor correlations:\\n\")\n",
    "print(correlations.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1238df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Initialization with predefined MultiIndex columns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Copy your data\n",
    "df = df_merged_clean.copy()\n",
    "\n",
    "# Define hub groups\n",
    "hub_1 = ['T_15_pv1', 'T_15_pv2', 'T_17_pv1', 'T_17_pv2',\n",
    "         'T_19_pv1', 'T_19_pv2', 'T_14_pv1', 'T_14_pv2']\n",
    "hub_2 = ['T_16_pv1', 'T_16_pv2', 'T_13_pv1', 'T_13_pv2',\n",
    "         'T_18_pv1', 'T_18_pv2', 'T_20_pv1', 'T_20_pv2']\n",
    "hub_3 = ['T_11_pv1', 'T_11_pv2', 'T_12_pv1',\n",
    "         'T_12_pv2', 'T_27_pv1', 'T_27_pv2']\n",
    "\n",
    "sensor_to_features = {}\n",
    "for s in hub_1:\n",
    "    sensor_to_features[s] = hub_2 + hub_3\n",
    "for s in hub_2:\n",
    "    sensor_to_features[s] = hub_1 + hub_3\n",
    "for s in hub_3:\n",
    "    sensor_to_features[s] = hub_1 + hub_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1) Prepare a dict to hold each sensor's trained LinearRegression model\n",
    "linear_models = {}\n",
    "\n",
    "# 2) Prepare a list to collect metrics in the same style as other models\n",
    "records_linear = []\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"LinearRegression\", unit=\"sensor\"):\n",
    "    # a) Extract features X and target y\n",
    "    X, y = df[feats], df[target]\n",
    "\n",
    "    # b) Perform the same 80/20 time‐ordered split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # c) Fit the model on the 80% training data\n",
    "    lr = LinearRegression().fit(X_tr, y_tr)\n",
    "\n",
    "    # d) Save the fitted model object under this sensor name\n",
    "    linear_models[target] = lr\n",
    "\n",
    "    # e) Compute predictions on the 20% test set (for metrics)\n",
    "    y_pred = lr.predict(X_te)\n",
    "\n",
    "    # f) Record RMSE/R² exactly as before\n",
    "    records_linear.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": {},  # no hyperparameters for plain LinearRegression\n",
    "        \"RMSE\":       np.sqrt(mean_squared_error(y_te, y_pred)),\n",
    "        \"R²\":         r2_score(y_te, y_pred)\n",
    "    })\n",
    "\n",
    "# 3) Build the summary DataFrame of metrics\n",
    "df_linear = pd.DataFrame(records_linear).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff83ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Save the trained models to disk using pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Choose a filename for the serialized models\n",
    "model_filename = \"linear_models.pkl\"\n",
    "\n",
    "# Dump the entire dictionary of models to a file\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(linear_models, f)\n",
    "\n",
    "print(f\"Saved {len(linear_models)} models to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_linear[[\"RMSE\", \"R²\"]]), df_linear[[\"RMSE\", \"R²\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sweep range\n",
    "alphas = np.logspace(-5, 3.5, 1000)\n",
    "\n",
    "# Output containers\n",
    "records = []\n",
    "ridge_models = {}  # Stores best Ridge model per sensor\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"Ridge\", unit=\"sensor\"):\n",
    "    X, y = df[feats], df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    best = {\"alpha\": None, \"rmse\": np.inf, \"r2\": None, \"model\": None}\n",
    "    for a in tqdm(alphas, desc=f\"  α-loop for {target}\", leave=False, unit=\"α\"):\n",
    "        model = Ridge(alpha=a).fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_te)\n",
    "        rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "        if rmse < best[\"rmse\"]:\n",
    "            best.update(alpha=a, rmse=rmse, r2=r2_score(\n",
    "                y_te, pred), model=model)\n",
    "\n",
    "    # Save model\n",
    "    ridge_models[target] = best[\"model\"]\n",
    "\n",
    "    # Log metrics\n",
    "    records.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\"alpha\": best[\"alpha\"]},\n",
    "        \"RMSE\": best[\"rmse\"],\n",
    "        \"R²\": best[\"r2\"]\n",
    "    })\n",
    "\n",
    "# Results DataFrame\n",
    "df_ridge = pd.DataFrame(records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all Ridge models into a single file\n",
    "with open(\"ridge_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ridge_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge, df_ridge.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322bd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameter grid\n",
    "alphas = np.logspace(-5, 3.5, 100)\n",
    "\n",
    "# Containers\n",
    "records = []\n",
    "ridge_poly2_tscv_models = {}\n",
    "\n",
    "# Cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"Ridge + Poly2 + TSCV\", unit=\"sensor\"):\n",
    "    X, y = df[feats].values, df[target].values\n",
    "\n",
    "    best = {\"alpha\": None, \"rmse\": np.inf, \"r2\": None, \"model\": None}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        pipe = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"ridge\", Ridge(alpha=alpha))\n",
    "        ])\n",
    "\n",
    "        y_preds = np.zeros_like(y, dtype=float)\n",
    "        y_preds[:] = np.nan\n",
    "\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_tr, X_te = X[train_idx], X[test_idx]\n",
    "            y_tr = y[train_idx]\n",
    "\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            y_preds[test_idx] = pipe.predict(X_te)\n",
    "\n",
    "        valid_mask = ~np.isnan(y_preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y[valid_mask], y_preds[valid_mask]))\n",
    "        r2 = r2_score(y[valid_mask], y_preds[valid_mask])\n",
    "\n",
    "        if rmse < best[\"rmse\"]:\n",
    "            best.update(alpha=alpha, rmse=rmse, r2=r2, model=pipe)\n",
    "\n",
    "    # Final refit on full data\n",
    "    best[\"model\"].fit(X, y)\n",
    "    ridge_poly2_tscv_models[target] = best[\"model\"]\n",
    "\n",
    "    records.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\"alpha\": best[\"alpha\"]},\n",
    "        \"RMSE\": best[\"rmse\"],\n",
    "        \"R²\": best[\"r2\"]\n",
    "    })\n",
    "\n",
    "df_ridge_poly2_tscv = pd.DataFrame(records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7881b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all models to a .pkl file\n",
    "with open(\"ridge_poly2_tscv_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ridge_poly2_tscv_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge_poly2_tscv, df_ridge_poly2_tscv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "records_knn = []\n",
    "models_knn = {}\n",
    "\n",
    "neighbors = range(1, 11)\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"KNN\", unit=\"sensor\"):\n",
    "    X, y = df[feats], df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    best = {\"k\": None, \"w\": None, \"rmse\": np.inf, \"r2\": None, \"model\": None}\n",
    "\n",
    "    for k in neighbors:\n",
    "        for w in weights:\n",
    "            pipe = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"knn\", KNeighborsRegressor(n_neighbors=k, weights=w))\n",
    "            ])\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            pred = pipe.predict(X_te)\n",
    "            rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "            r2 = r2_score(y_te, pred)\n",
    "\n",
    "            if rmse < best[\"rmse\"]:\n",
    "                best.update(k=k, w=w, rmse=rmse, r2=r2, model=pipe)\n",
    "\n",
    "    models_knn[target] = best[\"model\"]\n",
    "    records_knn.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\"k\": best[\"k\"], \"weights\": best[\"w\"]},\n",
    "        \"RMSE\": best[\"rmse\"],\n",
    "        \"R²\": best[\"r2\"]\n",
    "    })\n",
    "\n",
    "df_knn = pd.DataFrame(records_knn).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dictionary of best models\n",
    "knn_model_filename = \"models_knn.pkl\"\n",
    "\n",
    "with open(knn_model_filename, \"wb\") as f:\n",
    "    pickle.dump(models_knn, f)\n",
    "\n",
    "print(f\"Saved {len(models_knn)} KNN models to '{knn_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn, df_knn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "model_dict_rf = {}\n",
    "\n",
    "ests = [100, 200]\n",
    "depths = [10, 20]\n",
    "feats_opts = [0.65, 0.85]\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"RandomForest\", unit=\"sensor\"):\n",
    "    X, y = df[feats], df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "    best = {\"params\": None, \"rmse\": np.inf, \"r2\": None, \"model\": None}\n",
    "\n",
    "    for n in ests:\n",
    "        for d in depths:\n",
    "            for f in feats_opts:\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=n,\n",
    "                    max_depth=d,\n",
    "                    max_features=f,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                ).fit(X_tr, y_tr)\n",
    "\n",
    "                pred = model.predict(X_te)\n",
    "                rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "                r2 = r2_score(y_te, pred)\n",
    "\n",
    "                if rmse < best[\"rmse\"]:\n",
    "                    best.update(params={\"n_estimators\": n, \"max_depth\": d, \"max_features\": f},\n",
    "                                rmse=rmse, r2=r2, model=model)\n",
    "\n",
    "    model_dict_rf[target] = best[\"model\"]\n",
    "\n",
    "    records.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": best[\"params\"],\n",
    "        \"RMSE\": best[\"rmse\"],\n",
    "        \"R²\": best[\"r2\"]\n",
    "    })\n",
    "\n",
    "df_rf = pd.DataFrame(records).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Pickle the trained RandomForest models\n",
    "import pickle\n",
    "\n",
    "# Save the dictionary of best RF models\n",
    "rf_model_filename = \"models_rf.pkl\"\n",
    "\n",
    "with open(rf_model_filename, \"wb\") as f:\n",
    "    pickle.dump(model_dict_rf, f)\n",
    "\n",
    "print(f\"Saved {len(model_dict_rf)} RandomForest models to '{rf_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023783ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the dict‐column into separate columns\n",
    "df_rf_params = pd.json_normalize(df_rf[\"Parameters\"]).set_index(df_rf.index)\n",
    "\n",
    "# If you want to merge it back into df_rf:\n",
    "df_rf_expanded = df_rf.drop(columns=\"Parameters\").join(df_rf_params)\n",
    "\n",
    "# Display it:\n",
    "print(df_rf_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Containers\n",
    "records_mlp = []\n",
    "mlp_models = {}\n",
    "\n",
    "# You can tweak these MLP hyperparameters as needed:\n",
    "mlp_config = {\n",
    "    \"hidden_layer_sizes\": (100,),  # one hidden layer of 100 neurons\n",
    "    \"activation\": \"relu\",\n",
    "    \"alpha\": 0.0001,               # L2 penalty\n",
    "    \"learning_rate_init\": 0.001,\n",
    "    \"max_iter\": 1000\n",
    "}\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"MLP Regressor\", unit=\"sensor\"):\n",
    "    # 1) Split train/test (80/20 time‐ordered)\n",
    "    X, y = df[feats], df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # 2) Build pipeline: scale → MLP\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\", MLPRegressor(**mlp_config))\n",
    "    ])\n",
    "\n",
    "    # 3) Fit on the 80% training portion\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # 4) Evaluate on 20% hold‐out\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "    r2 = r2_score(y_te, y_pred)\n",
    "\n",
    "    # 5) Save the fitted pipeline and metrics\n",
    "    mlp_models[target] = pipe\n",
    "    records_mlp.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": mlp_config.copy(),\n",
    "        \"RMSE\":       rmse,\n",
    "        \"R²\":         r2\n",
    "    })\n",
    "\n",
    "# 6) Create a DataFrame of metrics\n",
    "df_mlp = pd.DataFrame(records_mlp).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Map each sensor → its Volt_H column\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "volt_map = {}\n",
    "for s in hub_1:\n",
    "    volt_map[s] = \"Volt_H1\"\n",
    "for s in hub_2:\n",
    "    volt_map[s] = \"Volt_H2\"\n",
    "for s in hub_3:\n",
    "    volt_map[s] = \"Volt_H3\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) MLP configuration (two hidden layers + early stopping)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "records_mlp = []\n",
    "mlp_models = {}\n",
    "\n",
    "mlp_config = {\n",
    "    \"hidden_layer_sizes\": (100, 50),    # two layers: 100 → 50\n",
    "    \"activation\": \"relu\",\n",
    "    \"alpha\": 0.0001,\n",
    "    \"learning_rate_init\": 0.001,\n",
    "    \"max_iter\": 500,\n",
    "    \"early_stopping\": True,\n",
    "    \"validation_fraction\": 0.1,\n",
    "    \"n_iter_no_change\": 20,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"MLP Regressor\", unit=\"sensor\"):\n",
    "    # 3) Build X by combining:\n",
    "    #    • hub‐outside sensors (feats)\n",
    "    #    • the correct Volt_H# column\n",
    "    #    • the Hour column\n",
    "    X_base = df[feats]\n",
    "    volt_col = volt_map[target]\n",
    "    X = pd.concat([\n",
    "        X_base,\n",
    "        df[[volt_col]],\n",
    "        df[[\"Hour\"]]\n",
    "    ], axis=1)\n",
    "\n",
    "    y = df[target]\n",
    "\n",
    "    # 4) Time‐ordered 80/20 split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # 5) Pipeline: StandardScaler → MLP\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\",    MLPRegressor(**mlp_config))\n",
    "    ])\n",
    "\n",
    "    # 6) Train on 80% portion\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # 7) Evaluate on 20% hold‐out\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "    r2 = r2_score(y_te, y_pred)\n",
    "\n",
    "    # 8) Save model and metrics\n",
    "    mlp_models[target] = pipe\n",
    "    records_mlp.append({\n",
    "        \"Target\":     target,\n",
    "        \"Parameters\": {\"Volt\": volt_col, \"Hour\": True, **mlp_config},\n",
    "        \"RMSE\":       rmse,\n",
    "        \"R²\":         r2\n",
    "    })\n",
    "\n",
    "# 9) DataFrame of metrics\n",
    "df_mlp = pd.DataFrame(records_mlp).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mlp[['RMSE', 'R²']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155337ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameter grid\n",
    "Cs = np.logspace(0, 4, 10)        # 10 values from 10^0 to 10^4\n",
    "epsilons = [0.01, 0.05, 0.1]\n",
    "gammas = [\"scale\"]                # you can also add \"auto\" if desired\n",
    "\n",
    "# Containers for metrics and best models\n",
    "records_svr = []\n",
    "svr_models = {}\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"SVR\", unit=\"sensor\"):\n",
    "    # Prepare feature matrix and target vector\n",
    "    X, y = df[feats], df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    best = {\"C\": None, \"epsilon\": None, \"gamma\": None,\n",
    "            \"rmse\": np.inf, \"r2\": None, \"model\": None}\n",
    "\n",
    "    # Grid search over (C, ε, γ)\n",
    "    for C in Cs:\n",
    "        for epsilon in epsilons:\n",
    "            for gamma in gammas:\n",
    "                pipe = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"svr\", SVR(kernel=\"rbf\", C=C, epsilon=epsilon, gamma=gamma))\n",
    "                ])\n",
    "                pipe.fit(X_tr, y_tr)\n",
    "                y_pred = pipe.predict(X_te)\n",
    "                rmse_val = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "                r2_val = r2_score(y_te, y_pred)\n",
    "\n",
    "                if rmse_val < best[\"rmse\"]:\n",
    "                    best.update(C=C, epsilon=epsilon, gamma=gamma,\n",
    "                                rmse=rmse_val, r2=r2_val, model=pipe)\n",
    "\n",
    "    # Save best pipeline for this sensor\n",
    "    svr_models[target] = best[\"model\"]\n",
    "\n",
    "    # Record metrics\n",
    "    records_svr.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\"C\": best[\"C\"], \"epsilon\": best[\"epsilon\"], \"gamma\": best[\"gamma\"]},\n",
    "        \"RMSE\": best[\"rmse\"],\n",
    "        \"R²\": best[\"r2\"]\n",
    "    })\n",
    "\n",
    "# Build DataFrame of results\n",
    "df_svr = pd.DataFrame(records_svr).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Save the trained SVR models to disk using pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "svr_model_filename = \"models_svr.pkl\"\n",
    "with open(svr_model_filename, \"wb\") as f:\n",
    "    pickle.dump(svr_models, f)\n",
    "\n",
    "print(f\"Saved {len(svr_models)} SVR models to '{svr_model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a12ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_svr[[\"RMSE\", \"R²\"]]), print(df_svr[[\"RMSE\", \"R²\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Train GradientBoostingRegressor over a small hyperparameter grid and save the best per sensor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameter grid\n",
    "n_estimators_options = [100, 200, 400]\n",
    "max_depth_options = [3, 5, 8]\n",
    "learning_rate_options = [0.01, 0.05, 0.1]\n",
    "\n",
    "records_gb = []\n",
    "gb_models = {}\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"GBRT Training\", unit=\"sensor\"):\n",
    "    # 1) Prepare features and target, then split 80% train / 20% test (time‐ordered)\n",
    "    X, y = df[feats], df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    best = {\"n_estimators\": None, \"max_depth\": None,\n",
    "            \"learning_rate\": None, \"rmse\": np.inf, \"r2\": None, \"model\": None}\n",
    "\n",
    "    # 2) Grid‐search over (n_estimators, max_depth, learning_rate)\n",
    "    for n_est in n_estimators_options:\n",
    "        for md in max_depth_options:\n",
    "            for lr in learning_rate_options:\n",
    "                pipe = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"gbrt\", GradientBoostingRegressor(\n",
    "                        n_estimators=n_est,\n",
    "                        max_depth=md,\n",
    "                        learning_rate=lr,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])\n",
    "                pipe.fit(X_tr, y_tr)\n",
    "                y_pred = pipe.predict(X_te)\n",
    "                rmse_val = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "                r2_val = r2_score(y_te, y_pred)\n",
    "\n",
    "                if rmse_val < best[\"rmse\"]:\n",
    "                    best.update(\n",
    "                        n_estimators=n_est,\n",
    "                        max_depth=md,\n",
    "                        learning_rate=lr,\n",
    "                        rmse=rmse_val,\n",
    "                        r2=r2_val,\n",
    "                        model=pipe\n",
    "                    )\n",
    "\n",
    "    # 3) Save the best pipeline and its metrics for this sensor\n",
    "    gb_models[target] = best[\"model\"]\n",
    "    records_gb.append({\n",
    "        \"Target\": target,\n",
    "        \"Parameters\": {\n",
    "            \"n_estimators\": best[\"n_estimators\"],\n",
    "            \"max_depth\": best[\"max_depth\"],\n",
    "            \"learning_rate\": best[\"learning_rate\"]\n",
    "        },\n",
    "        \"RMSE\": best[\"rmse\"],\n",
    "        \"R²\": best[\"r2\"]\n",
    "    })\n",
    "\n",
    "# 4) Create DataFrame of results\n",
    "df_gb = pd.DataFrame(records_gb).set_index(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cf5ad",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ASSUMPTION:\n",
    "# • You have a dict `models` where each key is a sensor name and each value\n",
    "#   is its fitted LinearRegression (trained on df_merged_clean[feats] DataFrames).\n",
    "# • You have df_merged_clean and sensor_to_features defined exactly as before.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"Predict from Saved Models\", unit=\"sensor\"):\n",
    "    # Instead of pulling out .values, keep it as a DataFrame:\n",
    "    # <-- this is a DataFrame with column names\n",
    "    X_full_df = df_merged_clean[feats]\n",
    "\n",
    "    # Grab the pre‐trained LinearRegression for this sensor:\n",
    "    lr_model = models[target]\n",
    "\n",
    "    # Now predict using the DataFrame directly (no .values):\n",
    "    y_pred = lr_model.predict(X_full_df)\n",
    "\n",
    "    predictions[target] = y_pred\n",
    "\n",
    "# Rebuild the merged‐predictions DataFrame exactly as before:\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_linear_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "for s in hub_1:\n",
    "    df_linear_merged_clean[s] = predictions[s]\n",
    "for s in hub_2:\n",
    "    df_linear_merged_clean[s] = predictions[s]\n",
    "for s in hub_3:\n",
    "    df_linear_merged_clean[s] = predictions[s]\n",
    "\n",
    "# Reorder columns to match the original\n",
    "df_linear_merged_clean = df_linear_merged_clean[df_merged_clean.columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assumes:\n",
    "# - df_merged_clean is defined and cleaned\n",
    "# - ridge_models contains trained Ridge models per sensor\n",
    "# - sensor_to_features, hub_1, hub_2, hub_3 are defined\n",
    "\n",
    "# 1) Container for predictions\n",
    "ridge_predictions = {}\n",
    "\n",
    "# 2) Predict all sensors using stored Ridge models\n",
    "for target, feats in tqdm(sensor_to_features.items(), desc=\"Predicting (Ridge)\", unit=\"sensor\"):\n",
    "    # Keep column names for sklearn compatibility\n",
    "    X_full = df_merged_clean[feats]\n",
    "    model = ridge_models[target]\n",
    "    ridge_predictions[target] = model.predict(X_full)\n",
    "\n",
    "# 3) Initialize with non-sensor columns\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_ridge_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "# 4) Add predictions in same order as original columns\n",
    "for s in hub_1:\n",
    "    df_ridge_merged_clean[s] = ridge_predictions[s]\n",
    "for s in hub_2:\n",
    "    df_ridge_merged_clean[s] = ridge_predictions[s]\n",
    "for s in hub_3:\n",
    "    df_ridge_merged_clean[s] = ridge_predictions[s]\n",
    "\n",
    "# 5) Reorder to match original structure exactly\n",
    "desired_order = list(df_merged_clean.columns)\n",
    "df_ridge_merged_clean = df_ridge_merged_clean[desired_order].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64156018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge_poly2_tscv_merged_clean = df_merged_clean[[\n",
    "    \"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]].copy()\n",
    "\n",
    "for group in [hub_1, hub_2, hub_3]:\n",
    "    for sensor in group:\n",
    "        feats = sensor_to_features[sensor]\n",
    "        X_full = df_merged_clean[feats]\n",
    "        model = models_ridge_poly2_tscv[sensor]\n",
    "        df_ridge_poly2_tscv_merged_clean[sensor] = model.predict(X_full)\n",
    "\n",
    "# Reorder columns to match original df_merged_clean\n",
    "df_ridge_poly2_tscv_merged_clean = df_ridge_poly2_tscv_merged_clean[df_merged_clean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Create predicted version of df_merged_clean using trained KNN models\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "predictions_knn = {}\n",
    "\n",
    "for sensor in tqdm(models_knn.keys(), desc=\"Predict (KNN)\", unit=\"sensor\"):\n",
    "    X_full = df_merged_clean[sensor_to_features[sensor]]\n",
    "    model = models_knn[sensor]\n",
    "    predictions_knn[sensor] = model.predict(X_full)\n",
    "\n",
    "# Prepare new DataFrame\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_knn_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "for s in hub_1 + hub_2 + hub_3:\n",
    "    df_knn_merged_clean[s] = predictions_knn[s]\n",
    "\n",
    "# Reorder to match original\n",
    "df_knn_merged_clean = df_knn_merged_clean[df_merged_clean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on full data with best fitted models\n",
    "predictions = {}\n",
    "\n",
    "for sensor, model in model_dict_rf.items():\n",
    "    X_full = df_merged_clean[sensor_to_features[sensor]]\n",
    "    predictions[sensor] = model.predict(X_full)\n",
    "\n",
    "# Base structure\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_rf_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "# Insert in hub order\n",
    "for s in hub_1:\n",
    "    df_rf_merged_clean[s] = predictions[s]\n",
    "for s in hub_2:\n",
    "    df_rf_merged_clean[s] = predictions[s]\n",
    "for s in hub_3:\n",
    "    df_rf_merged_clean[s] = predictions[s]\n",
    "\n",
    "# Final reordering\n",
    "df_rf_merged_clean = df_rf_merged_clean[df_merged_clean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacd986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Empty DataFrame with non‐sensor columns copied over\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_mlp_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "# 2) Predict each sensor's full series using the saved MLP pipelines\n",
    "for sensor in tqdm(mlp_models.keys(), desc=\"Predicting (MLP)\", unit=\"sensor\"):\n",
    "    feats = sensor_to_features[sensor]\n",
    "    X_full = df_merged_clean[feats]\n",
    "\n",
    "    # Use DataFrame directly so scaler sees column names\n",
    "    pipe = mlp_models[sensor]\n",
    "    df_mlp_merged_clean[sensor] = pipe.predict(X_full)\n",
    "\n",
    "# 3) Re‐order columns to match original df_merged_clean\n",
    "df_mlp_merged_clean = df_mlp_merged_clean[df_merged_clean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Use saved SVR pipelines to predict full‐series and build df_svr_merged_clean\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Copy non‐sensor columns from df_merged_clean\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_svr_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "# 2) Predict each sensor's entire time series using its SVR pipeline\n",
    "for sensor, model in tqdm(svr_models.items(), desc=\"Predicting (SVR)\", unit=\"sensor\"):\n",
    "    feats = sensor_to_features[sensor]\n",
    "    X_full = df_merged_clean[feats]  # keep DataFrame with column names\n",
    "    df_svr_merged_clean[sensor] = model.predict(X_full)\n",
    "\n",
    "# 3) Re‐order to match original df_merged_clean\n",
    "df_svr_merged_clean = df_svr_merged_clean[df_merged_clean.columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492206e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Use saved GBRT pipelines to predict full‐series and build df_gb_merged_clean\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Copy non‐sensor columns from df_merged_clean\n",
    "cols_non_sensors = [\"Datetime\", \"Volt_H1\", \"Volt_H2\", \"Volt_H3\", \"Hour\"]\n",
    "df_gb_merged_clean = df_merged_clean[cols_non_sensors].copy()\n",
    "\n",
    "# 2) Predict each sensor's entire time series using its GBRT pipeline\n",
    "for sensor, model in tqdm(gb_models.items(), desc=\"Predicting (GBRT)\", unit=\"sensor\"):\n",
    "    feats = sensor_to_features[sensor]\n",
    "    X_full = df_merged_clean[feats]  # preserve column names for scaler\n",
    "    df_gb_merged_clean[sensor] = model.predict(X_full)\n",
    "\n",
    "# 3) Re‐order to match original df_merged_clean\n",
    "df_gb_merged_clean = df_gb_merged_clean[df_merged_clean.columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7495daf",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_prediction_vs_measured_all(df_measured, df_predicted, sensors, df_stats):\n",
    "    \"\"\"\n",
    "    Plots measured vs predicted values for all sensors in the list.\n",
    "\n",
    "    Parameters:\n",
    "    - df_measured: DataFrame with actual sensor values.\n",
    "    - df_predicted: DataFrame with predicted sensor values.\n",
    "    - sensors: list of column names to compare.\n",
    "    - df_stats: DataFrame with RMSE and R² (e.g., df_linear), indexed by sensor.\n",
    "    \"\"\"\n",
    "    for sensor in sensors:\n",
    "        measured = df_measured[sensor].values\n",
    "        predicted = df_predicted[sensor].values\n",
    "\n",
    "        # Pull RMSE and R² from stats DataFrame\n",
    "        rmse_val = df_stats.loc[sensor, \"RMSE\"]\n",
    "        r2_val = df_stats.loc[sensor, \"R²\"]\n",
    "\n",
    "        # Annotation text with only R² (Model) and RMSE\n",
    "        annotation_text = (\n",
    "            f\"R² = {r2_val:.3f}<br>RMSE = {rmse_val:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Create scatter plot\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=measured,\n",
    "            y=predicted,\n",
    "            mode=\"markers\",\n",
    "            name=\"Data\",\n",
    "            marker=dict(size=4, color=\"#196EE6\", opacity=0.6)\n",
    "        ))\n",
    "\n",
    "        # Add y = x line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=measured,\n",
    "            y=measured,\n",
    "            mode=\"lines\",\n",
    "            name=\"y = x\",\n",
    "            line=dict(color=\"gray\", dash=\"dash\")\n",
    "        ))\n",
    "\n",
    "        # Annotate stats\n",
    "        fig.add_annotation(\n",
    "            text=annotation_text,\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.98, y=0.02,\n",
    "            showarrow=False,\n",
    "            font=dict(size=13),\n",
    "            align=\"right\"\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Measured vs Predicted for {sensor}\",\n",
    "            xaxis_title=\"Measured\",\n",
    "            yaxis_title=\"Predicted\",\n",
    "            width=650,\n",
    "            height=650,\n",
    "            template=\"plotly_white\",\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\",\n",
    "                        y=1.02, xanchor=\"right\", x=1)\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da353f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sensor with best R², lowest RMSE, worst R², and highest RMSE\n",
    "best_r2_sensor = df_linear[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_linear[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_linear[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_linear[\"RMSE\"].idxmax()\n",
    "\n",
    "# Build unique list (avoid duplicates if same sensor satisfies multiple criteria)\n",
    "selected_sensors = list(\n",
    "    {best_r2_sensor, lowest_rmse_sensor, worst_r2_sensor, highest_rmse_sensor})\n",
    "\n",
    "# # Plot them using existing function\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_linear_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_linear\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c37b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sensor with best R², lowest RMSE, worst R², and highest RMSE for Ridge\n",
    "best_r2_sensor = df_ridge[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_ridge[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_ridge[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_ridge[\"RMSE\"].idxmax()\n",
    "\n",
    "# Unique list of sensors to plot\n",
    "selected_sensors = list({\n",
    "    best_r2_sensor,\n",
    "    lowest_rmse_sensor,\n",
    "    worst_r2_sensor,\n",
    "    highest_rmse_sensor\n",
    "})\n",
    "\n",
    "# # Plot them using the same plot function\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_ridge_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_ridge\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2989740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify extremes\n",
    "best_r2_sensor = df_ridge_poly2_tscv[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_ridge_poly2_tscv[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_ridge_poly2_tscv[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_ridge_poly2_tscv[\"RMSE\"].idxmax()\n",
    "\n",
    "selected_sensors = list({\n",
    "    best_r2_sensor,\n",
    "    lowest_rmse_sensor,\n",
    "    worst_r2_sensor,\n",
    "    highest_rmse_sensor\n",
    "})\n",
    "\n",
    "# # Plot\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_ridge_poly2_tscv_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_ridge_poly2_tscv\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find extreme sensors based on df_knn\n",
    "best_r2_sensor = df_knn[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_knn[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_knn[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_knn[\"RMSE\"].idxmax()\n",
    "\n",
    "selected_sensors = list(\n",
    "    {best_r2_sensor, lowest_rmse_sensor, worst_r2_sensor, highest_rmse_sensor})\n",
    "\n",
    "# # Use your existing function to visualize\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_knn_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_knn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best/worst selectors\n",
    "best_r2_sensor = df_rf[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_rf[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_rf[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_rf[\"RMSE\"].idxmax()\n",
    "\n",
    "selected_sensors = list(\n",
    "    {best_r2_sensor, lowest_rmse_sensor, worst_r2_sensor, highest_rmse_sensor})\n",
    "\n",
    "# # Use previously defined plot function (assumed to be `plot_prediction_vs_measured_all`)\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_rf_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_rf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Identify extremes\n",
    "best_r2_sensor = df_mlp[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_mlp[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_mlp[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_mlp[\"RMSE\"].idxmax()\n",
    "\n",
    "selected_sensors = list({\n",
    "    best_r2_sensor,\n",
    "    lowest_rmse_sensor,\n",
    "    worst_r2_sensor,\n",
    "    highest_rmse_sensor\n",
    "})\n",
    "\n",
    "# # 2) Plot them\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_mlp_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_mlp\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd93fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Plot best/worst‐performing sensors for SVR using existing plot function\n",
    "\n",
    "# 1) Identify extremes in df_svr\n",
    "best_r2_sensor = df_svr[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_svr[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_svr[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_svr[\"RMSE\"].idxmax()\n",
    "\n",
    "selected_sensors = list({\n",
    "    best_r2_sensor,\n",
    "    lowest_rmse_sensor,\n",
    "    worst_r2_sensor,\n",
    "    highest_rmse_sensor\n",
    "})\n",
    "\n",
    "# # 2) Plot measured vs predicted for those sensors\n",
    "# plot_prediction_vs_measured_all(\n",
    "#     df_measured=df_merged_clean,\n",
    "#     df_predicted=df_svr_merged_clean,\n",
    "#     sensors=selected_sensors,\n",
    "#     df_stats=df_svr\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c11870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Plot best/worst‐performing sensors for GBRT using existing plot function\n",
    "\n",
    "# 1) Identify extremes in df_gb\n",
    "best_r2_sensor = df_gb[\"R²\"].idxmax()\n",
    "lowest_rmse_sensor = df_gb[\"RMSE\"].idxmin()\n",
    "worst_r2_sensor = df_gb[\"R²\"].idxmin()\n",
    "highest_rmse_sensor = df_gb[\"RMSE\"].idxmax()\n",
    "\n",
    "selected_sensors = list({\n",
    "    best_r2_sensor,\n",
    "    lowest_rmse_sensor,\n",
    "    worst_r2_sensor,\n",
    "    highest_rmse_sensor\n",
    "})\n",
    "\n",
    "# 2) Plot measured vs predicted for those sensors\n",
    "plot_prediction_vs_measured_all(\n",
    "    df_measured=df_merged_clean,\n",
    "    df_predicted=df_gb_merged_clean,\n",
    "    sensors=selected_sensors,\n",
    "    df_stats=df_gb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af051f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_residual_histogram(sensor, df_measured, df_predicted, nbins=40):\n",
    "    \"\"\"\n",
    "    Plot a histogram of residuals (Predicted − Measured) for a given sensor.\n",
    "    \n",
    "    Parameters:\n",
    "      • sensor: string, column name of the sensor to analyze\n",
    "      • df_measured: DataFrame containing the true 'Measured' series\n",
    "      • df_predicted: DataFrame containing the model 'Predicted' series\n",
    "      • nbins: integer, number of histogram bins (default: 40)\n",
    "    \"\"\"\n",
    "    # 1) Compute residuals\n",
    "    measured = df_measured[sensor].values\n",
    "    predicted = df_predicted[sensor].values\n",
    "    resid = predicted - measured\n",
    "\n",
    "    # 2) Compute statistics\n",
    "    mu = np.mean(resid)\n",
    "    sigma = np.std(resid, ddof=1)\n",
    "\n",
    "    # 3) Build histogram\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=resid,\n",
    "        nbinsx=nbins,\n",
    "        name=\"Residuals\",\n",
    "        marker_color=\"#196EE6\",\n",
    "        opacity=0.75\n",
    "    ))\n",
    "\n",
    "    # 4) Add a vertical line at zero\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[0, 0], y=[0, max(np.histogram(resid, bins=nbins)[0])],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", dash=\"dash\"),\n",
    "        name=\"Zero\"\n",
    "    ))\n",
    "\n",
    "    # 5) Annotate mean ± std\n",
    "    annotation_text = (\n",
    "        f\"mean = {mu:.3f}<br>σ = {sigma:.3f}\"\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=annotation_text,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.95, y=0.95,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        bgcolor=\"white\",\n",
    "        align=\"right\"\n",
    "    )\n",
    "\n",
    "    # 6) Layout\n",
    "    fig.update_layout(\n",
    "        title=f\"{sensor}: Residuals (Predicted − Measured)\",\n",
    "        xaxis_title=\"Residual value\",\n",
    "        yaxis_title=\"Count\",\n",
    "        template=\"plotly_white\",\n",
    "        width=700,\n",
    "        height=500,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\",\n",
    "                    y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a356a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svr_merged_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svr = df_svr[\"R²\"].idxmax()\n",
    "plot_residual_histogram(\n",
    "    sensor=best_svr,\n",
    "    df_measured=df_merged_clean,\n",
    "    df_predicted=df_svr_merged_clean,\n",
    "    nbins = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all sensors and show their SVR residual histograms\n",
    "all_sensors = df_svr.index.tolist()\n",
    "\n",
    "for sensor in all_sensors:\n",
    "    plot_residual_histogram(\n",
    "        sensor=sensor,\n",
    "        df_measured=df_merged_clean,\n",
    "        df_predicted=df_svr_merged_clean,\n",
    "        nbins=480\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bridges_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
